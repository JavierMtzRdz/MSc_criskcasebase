@book{AalenBorganGjessing:2008,
  title = {Survival and {{Event History Analysis}}},
  author = {Aalen, Odd O. and Borgan, {\O}rnulf and Gjessing, H{\aa}kon K.},
  editor = {Gail, M. and Krickeberg, K. and Samet, J. and Tsiatis, A. and Wong, W.},
  year = 2008,
  series = {Statistics for {{Biology}} and {{Health}}},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-68560-1},
  urldate = {2025-07-16},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-0-387-20287-7 978-0-387-68560-1},
  keywords = {causality,counting process,counting processes,cox regression model,frailty models,Markov process,Martingale,multivariate survival data,quality control reliability safety and risk,Radiologieinformationssystem,Sage,statistics,stochastic process,Stochastic processes},
  file = {/Users/javiermtz/Zotero/storage/NZ7W86PW/Aalen et al. - 2008 - Survival and Event History Analysis.pdf}
}

@article{AmoPerez-HoyosMoreno:2006,
  title = {Trends in {{AIDS}} and {{Mortality}} in {{HIV-Infected Subjects With Hemophilia From}} 1985 to 2003: {{The Competing Risks}} for {{Death Between AIDS}} and {{Liver Disease}}},
  shorttitle = {Trends in {{AIDS}} and {{Mortality}} in {{HIV-Infected Subjects With Hemophilia From}} 1985 to 2003},
  author = {del Amo, Julia and {P{\'e}rez-Hoyos}, Santiago and Moreno, Alicia and Quintana, Manuel and Ruiz, Isabel and Cisneros, Jos{\'e} Miguel and Ferreros, Inmaculada and Gonz{\'a}lez, Cristina and {de Olalla}, Patricia Garc{\'i}a and P{\'e}rez, Rosario and Hern{\'a}ndez, Ildefonso and of Seroconverters, Spanish Multicenter Study Group},
  year = 2006,
  month = apr,
  journal = {JAIDS Journal of Acquired Immune Deficiency Syndromes},
  volume = {41},
  number = {5},
  pages = {624},
  issn = {1525-4135},
  doi = {10.1097/01.qai.0000194232.85336.dc},
  urldate = {2025-07-09},
  abstract = {Objective:~           To study trends in progression to AIDS, all-cause mortality, and cause-specific mortality (AIDS-related, liver disease, and hemorrhagic complications) over calendar periods with different exposure to highly active antiretroviral therapy (HAART) in a cohort of hemophiliacs in Spain, taking into account the competing risks of the causes of death.           Methods:~           Multicenter cohort of HIV-infected hemophiliacs. HIV seroconversion was estimated using mathematic techniques for interval-censored data from 1979 through 1985. Rates of AIDS and cause-specific death were calculated by Poisson regression, allowing for late entry, for the periods 1985 through 1992, 1993 through 1996, 1997 through 2000 (early HAART), and 2001 through 2003 (late HAART), also allowing for competing risks.           Results:~           Of 585 subjects, 44\% were younger than 15 years of age, 82\% had severe hemophilia, 86\% had type A hemophilia, and the median seroconversion date was October 1982. Calendar period and age at HIV seroconversion strongly influenced AIDS and death rates. Compared with 1993 through 1996, decreases of 75\% (relative risk [RR] = 0.25, 95\% confidence interval [CI]: 0.14 to 0.43) and 72\% (RR = 0.28, 95\% CI: 0.12 to 0.63) in the RR of AIDS were observed in early and late HAART. For all-cause mortality, 72\% (RR = 0.28, 95\% CI: 0.18 to 0.42) and 83\% (RR = 0.17, 95\% CI: 0.09 to 0.33) decreases were observed by 1997 through 2000 and 2001 through 2003. For liver-related deaths, increases were observed in the late-HAART period (RR = 2.80, 95\% CI: 0.94 to 8.36) compared with 1993 through 1996, but using competing risks, this RR was substantially reduced (RR = 1.70, 95\% CI: 0.57 to 5.04).           Discussion:~           Major reductions in AIDS and death rates were observed from 1997 to 2003 in hemophiliacs. These survival improvements are largely attributable to decreases in AIDS-related deaths and have been accompanied by increases in liver disease death rates, which are overestimated if competing risks are not taken into account.},
  langid = {american},
  file = {/Users/javiermtz/Zotero/storage/WZVZBNFQ/trends_in_aids_and_mortality_in_hiv_infected.12.html}
}

@article{Anderson:1972,
  title = {Separate {{Sample Logistic Discrimination}}},
  author = {Anderson, J. A.},
  year = 1972,
  journal = {Biometrika},
  volume = {59},
  number = {1},
  eprint = {2334611},
  eprinttype = {jstor},
  pages = {19--35},
  publisher = {[Oxford University Press, Biometrika Trust]},
  issn = {0006-3444},
  doi = {10.2307/2334611},
  urldate = {2025-07-10},
  abstract = {The problem of discrimination when all or most of the observations are qualitative is discussed. The method of logistic discrimination introduced by Cox (1966) and Day \& Kerridge (1967) is extended to the situation where separate samples are taken from each population, using the results of Aitchison \& Silvey (1958) on constrained maximum likelihood estimation. The method is further extended to discrimination between three or more populations. The properties of logistic discrimination are investigated by simulation and the method is applied to the differential diagnosis of kerato-conjunctivitis sicca.}
}

@article{ArjasHaara:1987,
  title = {A {{Logistic Regression Model}} for {{Hazard}}: {{Asymptotic Results}}},
  shorttitle = {A {{Logistic Regression Model}} for {{Hazard}}},
  author = {Arjas, Elja and Haara, Pentti},
  year = 1987,
  journal = {Scandinavian Journal of Statistics},
  volume = {14},
  number = {1},
  eprint = {4616044},
  eprinttype = {jstor},
  pages = {1--18},
  publisher = {[Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]},
  issn = {0303-6898},
  urldate = {2025-07-16},
  abstract = {A dynamic form of the discrete time logistic regression model is considered as a means to analyse complicated failure time data. A characteristic property of this approach is that the events recorded in the data are always treated in the model in the order in which they occurred in real time, without first aligning them according to some particular "basic time measurement". Parametric modelling is used throughout. This paper deals with asymptotic results. The key theorems concern the asymptotic normality of the estimated regression coefficients and the limiting behaviour of the empirical score process as observation time tends to infinity.}
}

@article{BelloniChernozhukov:2013,
  title = {Least Squares after Model Selection in High-Dimensional Sparse Models},
  author = {Belloni, Alexandre and Chernozhukov, Victor},
  year = 2013,
  month = may,
  journal = {Bernoulli},
  volume = {19},
  number = {2},
  issn = {1350-7265},
  doi = {10.3150/11-BEJ410},
  urldate = {2025-10-31},
  file = {/Users/javiermtz/Zotero/storage/C39DGBD6/Belloni and Chernozhukov - 2013 - Least squares after model selection in high-dimensional sparse models.pdf}
}

@article{BelloniChernozhukovWei:2016,
  title = {Post-{{Selection Inference}} for {{Generalized Linear Models With Many Controls}}},
  author = {Belloni, Alexandre and Chernozhukov, Victor and Wei, Ying},
  year = 2016,
  month = oct,
  journal = {Journal of Business \& Economic Statistics},
  volume = {34},
  number = {4},
  pages = {606--619},
  issn = {0735-0015, 1537-2707},
  doi = {10.1080/07350015.2016.1166116},
  urldate = {2025-10-31},
  langid = {english},
  file = {/Users/javiermtz/Zotero/storage/64BLVE28/Belloni et al. - 2016 - Post-Selection Inference for Generalized Linear Models With Many Controls.pdf}
}

@book{BeyersmannAllignolSchumacher:2012,
  title = {Competing {{Risks}} and {{Multistate Models}} with {{R}}},
  author = {Beyersmann, Jan and Allignol, Arthur and Schumacher, Martin},
  year = 2012,
  publisher = {Springer},
  address = {New York},
  abstract = {This book covers competing risks and multistate models, sometimes summarized as event history analysis. These models generalize the analysis of time to a single event (survival analysis) to analysing the timing of distinct terminal events (competing risks) and possible intermediate events (multistate models). Both R and multistate methods are promoted with a focus on nonparametric methods.},
  isbn = {978-1-4614-2034-7},
  langid = {english}
}

@article{BhatnagarTurgeonIslam:2022,
  title = {Casebase: {{An Alternative Framework}} for {{Survival Analysis}} and {{Comparison}} of {{Event Rates}}},
  shorttitle = {Casebase},
  author = {Bhatnagar, Sahir Rai and Turgeon, Maxime and Islam, Jesse and Hanley, James A. and Saarela, Olli},
  year = 2022,
  month = dec,
  journal = {The R Journal},
  volume = {14},
  number = {3},
  pages = {59--79},
  issn = {2073-4859},
  doi = {10.32614/RJ-2022-052},
  urldate = {2025-06-19},
  abstract = {In clinical studies of time-to-event data, a quantity of interest to the clinician is their patient's risk of an event. However, methods relying on time matching or risk-set sampling (including Cox regression) eliminate the baseline hazard from the estimating function. As a consequence, the focus has been on reporting hazard ratios instead of survival or cumulative incidence curves. Indeed, reporting patient risk or cumulative incidence requires a separate estimation of the baseline hazard. Using case-base sampling, Hanley \& Miettinen (2009) explained how parametric hazard functions can be estimated in continuous-time using logistic regression. Their approach naturally leads to estimates of the survival or risk function that are smooth-in-time. In this paper, we present the casebase R package, a comprehensive and flexible toolkit for parametric survival analysis. We describe how the case-base framework can also be used in more complex settings: non-linear functions of time and non-proportional hazards, competing risks, and variable selection. Our package also includes an extensive array of visualization tools to complement the analysis. We illustrate all these features through three different case studies. * SRB and MT contributed equally to this work.},
  file = {/Users/javiermtz/Zotero/storage/D9RD5ZRP/Bhatnagar et al. - 2022 - casebase An Alternative Framework for Survival Analysis and Comparison of Event Rates.pdf}
}

@article{BinderAllignolSchumacher:2009,
  title = {Boosting for High-Dimensional Time-to-Event Data with Competing Risks},
  author = {Binder, Harald and Allignol, Arthur and Schumacher, Martin and Beyersmann, Jan},
  year = 2009,
  month = apr,
  journal = {Bioinformatics},
  volume = {25},
  number = {7},
  pages = {890--896},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btp088},
  urldate = {2025-07-09},
  abstract = {Motivation: For analyzing high-dimensional time-to-event data with competing risks, tailored modeling techniques are required that consider the event of interest and the competing events at the same time, while also dealing with censoring. For low-dimensional settings, proportional hazards models for the subdistribution hazard have been proposed, but an adaptation for high-dimensional settings is missing. In addition, tools for judging the prediction performance of fitted models have to be provided.Results: We propose a boosting approach for fitting proportional subdistribution hazards models for high-dimensional data, that can e.g. incorporate a large number of microarray features, while also taking clinical covariates into account. Prediction performance is evaluated using bootstrap.632+ estimates of prediction error curves, adapted for the competing risks setting. This is illustrated with bladder cancer microarray data, where simultaneous consideration of both, the event of interest and competing events, allows for judging the additional predictive power gained from incorporating microarray measurements.Availability: The proposed boosting approach is implemented in the R package CoxBoost and prediction error estimation in the package peperr, both available from CRAN.Contact: ~binderh@fdm.uni-freiburg.de},
  file = {/Users/javiermtz/Zotero/storage/M65SGVX9/Binder et al. - 2009 - Boosting for high-dimensional time-to-event data with competing risks.pdf}
}

@phdthesis{Colnet:2023,
  type = {These de Doctorat},
  title = {Generalizing a Causal Effect from a Trial to a Target Population : Methodological and Theoretical Contributions},
  shorttitle = {Generalizing a Causal Effect from a Trial to a Target Population},
  author = {Colnet, B{\'e}n{\'e}dicte},
  year = 2023,
  month = jun,
  urldate = {2025-09-04},
  abstract = {La m{\'e}decine moderne, aussi dite m{\'e}decine fond{\'e}e sur les preuves, place les essais contr{\^o}l{\'e}s randomis{\'e}s (ECRs) au premier plan de la preuve clinique. En effet, la randomisation permet une estimation de l'effet causal du traitement, d{\'e}passant la simple association ou corr{\'e}lation.Cependant, de plus en plus de limites sont trouv{\'e}es aux ECRs, du fait de leurs stricts crit{\`e}res d'{\'e}ligibilit{\'e}, des conditions de r{\'e}alisation, des p{\'e}riodes de temps trop restreintes qu'ils couvrent, ou encore de leur petite taille d'{\'e}chantillon.Toutes ces raisons entament ce que l'on appelle la validit{\'e} externe des r{\'e}sultats.L'utilisation de donn{\'e}es observationnelles ou dites de vie r{\'e}elle constitue une potentielle solution. Les autorit{\'e}s sanitaires comme le r{\'e}gulateur am{\'e}ricain (Food and Drug Administration) ou encore la Haute Autorit{\'e} de la Sant{\'e} (HAS) soutiennent ces nouvelles pratiques. Mais les donn{\'e}es de vie r{\'e}elle ne sont pas non plus une panac{\'e}e, car leur analyse repose sur des hypoth{\`e}ses non v{\'e}rifiables pour la plupart. Des travaux plus r{\'e}cents proposent de combiner les deux sources de donn{\'e}es, afin de renforcer les faiblesses de l'une par les forces de l'autre. Ainsi, cette th{\`e}se propose d'abord une revue de toutes les m{\'e}thodes existantes sur le sujet, que ce soit pour d{\'e}confondre une base de donn{\'e}es observationnelles {\`a} partir de donn{\'e}es exp{\'e}rimentales ou bien pour g{\'e}n{\'e}raliser {\`a} d'autres populations une {\'e}tude randomis{\'e}e. Ce travail de th{\`e}se propose ensuite d'approfondir ce dernier aspect, en utilisant la repr{\'e}sentativit{\'e} des donn{\'e}es de vie r{\'e}elle pour re-pond{\'e}rer les r{\'e}sultats d'un ECR. Cette th{\`e}se {\'e}tudie les propri{\'e}t{\'e}s th{\'e}oriques de ces m{\'e}thodes, telles que les propri{\'e}t{\'e}s d'estimation {\`a} taille finie ou asymptotique (biais et variance). Ces r{\'e}sultats permettent d'obtenir des recommandations pratiques pour la recherche clinique, notamment concernant la s{\'e}lection de covariables. Cette th{\`e}se propose {\'e}galement une analyse de sensibilit{\'e} lorsque les covariables sont partiellement ou totalement observ{\'e}es.La plupart des travaux existants d{\'e}finissent l'effet d'un traitement comme une diff{\'e}rence absolue. Pourtant, d'autres m{\'e}triques, comme le ratio, sont pr{\'e}f{\'e}r{\'e}es dans la recherche clinique. Par cons{\'e}quent, cette th{\`e}se ouvre {\'e}galement la voie {\`a} la g{\'e}n{\'e}ralisation de toutes les mesures causales, et non pas seulement de l'une d'entre elles. Ce faisant, nous relions la g{\'e}n{\'e}ralisation {\`a} une pr{\'e}occupation plut{\^o}t ancienne de la causalit{\'e}, {\`a} savoir la collapsibilit{\'e} d'une mesure. Nous proposons {\'e}galement une autre fa{\c c}on d'appr{\'e}hender ce que l'on appelle l'h{\'e}t{\'e}rog{\'e}n{\'e}it{\'e} d'un effet. Ceci nous permet de montrer que les m{\'e}thodes pour g{\'e}n{\'e}raliser un effet causal d{\'e}pendent de la nature de l'outcome (continu ou binaire) ainsi que de la nature de la mesure d'int{\'e}r{\^e}t (ratio ou diff{\'e}rence).Tous les travaux de cette th{\`e}se sont d{\'e}velopp{\'e}s en lien avec la recherche clinique, notamment via le consortium fran{\c c}ais de la Traumabase.},
  collaborator = {Josse, Julie and Scornet, Erwan and Varoquaux, Ga{\"e}l},
  copyright = {Licence Etalab},
  school = {Institut polytechnique de Paris},
  keywords = {314,Apprentissage automatique,Apprentissage statistique,Causal inference,External validity,Inference causale,Machine learning,Statistique mathematique,Validite Externe}
}

@article{Cox:1972,
  title = {Regression {{Models}} and {{Life-Tables}}},
  author = {Cox, David R.},
  year = 1972,
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {34},
  number = {2},
  pages = {187--202},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1972.tb00899.x},
  urldate = {2025-07-09},
  abstract = {The analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.},
  copyright = {{\copyright} 1972 The Authors},
  langid = {english},
  keywords = {accelerated life tests,age-specific failure rate,asymptotic theory,censored data,conditional inference,hazard function,life table,medical applications,product limit estimate,regression,reliability theory,two-sample rank tests},
  file = {/Users/javiermtz/Zotero/storage/WKMKWA42/j.2517-6161.1972.tb00899.html}
}

@article{DriggsEhrhardtSchonlieb:2022,
  title = {Accelerating Variance-Reduced Stochastic Gradient Methods},
  author = {Driggs, Derek and Ehrhardt, Matthias J. and Sch{\"o}nlieb, Carola-Bibiane},
  year = 2022,
  month = feb,
  journal = {Mathematical Programming},
  volume = {191},
  number = {2},
  pages = {671--715},
  issn = {1436-4646},
  doi = {10.1007/s10107-020-01566-2},
  urldate = {2025-05-19},
  abstract = {Variance reduction is a crucial tool for improving the slow convergence of stochastic gradient descent. Only a few variance-reduced methods, however, have yet been shown to directly benefit from Nesterov's acceleration techniques to match the convergence rates of accelerated gradient methods. Such approaches rely on ``negative momentum'', a technique for further variance reduction that is generally specific to the SVRG gradient estimator. In this work, we show for the first time that negative momentum is unnecessary for acceleration and develop a universal acceleration framework that allows all popular variance-reduced methods to achieve accelerated convergence rates. The constants appearing in these rates, including their dependence on the number of functions n, scale with the mean-squared-error and bias of the gradient estimator. In a series of numerical experiments, we demonstrate that versions of SAGA, SVRG, SARAH, and SARGE using our framework significantly outperform non-accelerated versions and compare favourably with algorithms using negative momentum.},
  langid = {english},
  keywords = {68Q25,90C06,90C15,90C25,90C30,90C60,Accelerated gradient descent,Calculus of Variations and Optimization,Computational Methods for Stochastic Equations,Continuous Optimization,Convex optimisation,Optimization,Particle Acceleration,Stochastic Learning and Adaptive Control,Stochastic optimisation,Variance reduction},
  file = {/Users/javiermtz/Zotero/storage/EI2CLGVV/Driggs et al. - 2022 - Accelerating variance-reduced stochastic gradient methods.pdf}
}

@article{DyrskjotKruhofferThykjaer:2004,
  title = {Gene Expression in the Urinary Bladder: A Common Carcinoma in Situ Gene Expression Signature Exists Disregarding Histopathological Classification},
  shorttitle = {Gene Expression in the Urinary Bladder},
  author = {Dyrskj{\o}t, Lars and Kruh{\o}ffer, Mogens and Thykjaer, Thomas and Marcussen, Niels and Jensen, Jens L. and M{\o}ller, Klaus and {\O}rntoft, Torben F.},
  year = 2004,
  month = jun,
  journal = {Cancer Research},
  volume = {64},
  number = {11},
  pages = {4040--4048},
  issn = {0008-5472},
  doi = {10.1158/0008-5472.CAN-03-3620},
  abstract = {The presence of carcinoma in situ (CIS) lesions in the urinary bladder is associated with a high risk of disease progression to a muscle invasive stage. In this study, we used microarray expression profiling to examine the gene expression patterns in superficial transitional cell carcinoma (sTCC) with surrounding CIS (13 patients), without surrounding CIS lesions (15 patients), and in muscle invasive carcinomas (mTCC; 13 patients). Hierarchical cluster analysis separated the sTCC samples according to the presence or absence of CIS in the surrounding urothelium. We identified a few gene clusters that contained genes with similar expression levels in transitional cell carcinoma (TCC) with surrounding CIS and invasive TCC. However, no close relationship between TCC with adjacent CIS and invasive TCC was observed using hierarchical cluster analysis. Expression profiling of a series of biopsies from normal urothelium and urothelium with CIS lesions from the same urinary bladder revealed that the gene expression found in sTCC with surrounding CIS is found also in CIS biopsies as well as in histologically normal samples adjacent to the CIS lesions. Furthermore, we also identified similar gene expression changes in mTCC samples. We used a supervised learning approach to build a 16-gene molecular CIS classifier. The classifier was able to classify sTCC samples according to the presence or absence of surrounding CIS with a high accuracy. This study demonstrates that a CIS gene expression signature is present not only in CIS biopsies but also in sTCC, mTCC, and, remarkably, in histologically normal urothelium from bladders with CIS. Identification of this expression signature could provide guidance for the selection of therapy and follow-up regimen in patients with early stage bladder cancer.},
  langid = {english},
  pmid = {15173019},
  keywords = {Biopsy,Carcinoma in Situ,Carcinoma Transitional Cell,Cluster Analysis,Gene Expression Profiling,Humans,Neoplasm Staging,Oligonucleotide Array Sequence Analysis,Urinary Bladder Neoplasms}
}

@article{Ferreira-GonzalezPermanyer-MiraldaDomingo-Salvany:2007,
  title = {Problems with Use of Composite End Points in Cardiovascular Trials: Systematic Review of Randomised Controlled Trials},
  shorttitle = {Problems with Use of Composite End Points in Cardiovascular Trials},
  author = {{Ferreira-Gonz{\'a}lez}, Ignacio and {Permanyer-Miralda}, Gaiet and {Domingo-Salvany}, Ant{\`o}nia and Busse, Jason W. and {Heels-Ansdell}, Diane and Montori, Victor M. and Akl, Elie A. and Bryant, Dianne M. and {Alonso-Coello}, Pablo and Alonso, Jordi and Worster, Andrew and Upadhye, Suneel and Jaeschke, Roman and Sch{\"u}nemann, Holger J. and {Pacheco-Huergo}, Valeria and Wu, Ping and Mills, Edward J. and Guyatt, Gordon H.},
  year = 2007,
  month = apr,
  journal = {BMJ},
  volume = {334},
  number = {7597},
  pages = {786},
  publisher = {British Medical Journal Publishing Group},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.39136.682083.AE},
  urldate = {2025-07-17},
  abstract = {Objective To explore the extent to which components of composite end points in randomised controlled trials vary in importance to patients, the frequency of events in the more and less important components, and the extent of variability in the relative risk reductions across components. Design Systematic review of randomised controlled trials. Data sources Cardiovascular randomised controlled trials published in the Lancet, Annals of Internal Medicine, Circulation, European Heart Journal, JAMA, and New England Journal of Medicine, from 1 January 2002 to 30 June 2003. Component end points of composite end points were categorised according to importance to patients as fatal, critical, major, moderate, or minor. Results Of 114 identified randomised controlled trials that included a composite end point of importance to patients, 68\% (n=77) reported complete component data for the primary composite end point; almost all (98\%; n=112) primary composite end points included a fatal end point. Of 84 composite end points for which component data were available, 54\% (n=45) showed large or moderate gradients in both importance to patients and magnitude of effect across components. When analysed by categories of importance to patients, the most important components were associated with lower event rates in the control group (medians of 3.3-3.7\% for fatal, critical, and major outcomes; 12.3\% for moderate outcomes; and 8.0\% for minor outcomes). Components of greater importance to patients were associated with smaller treatment effects than less important ones (relative risk reduction of 8\% for death and 33\% for components of minor importance to patients). Conclusion The use of composite end points in cardiovascular trials is frequently complicated by large gradients in importance to patients and in magnitude of the effect of treatment across component end points. Higher event rates and larger treatment effects associated with less important components may result in misleading impressions of the impact of treatment.},
  chapter = {Research},
  copyright = {{\copyright} BMJ Publishing Group Ltd 2007},
  langid = {english},
  pmid = {17403713},
  file = {/Users/javiermtz/Zotero/storage/4ZSWQAJA/Ferreira-González et al. - 2007 - Problems with use of composite end points in cardiovascular trials systematic review of randomised.pdf}
}

@article{FineGray:1999,
  title = {A {{Proportional Hazards Model}} for the {{Subdistribution}} of a {{Competing Risk}}},
  author = {Fine, Jason P. and Gray, Robert J.},
  year = 1999,
  journal = {Journal of the American Statistical Association},
  volume = {94},
  number = {446},
  eprint = {2670170},
  eprinttype = {jstor},
  pages = {496--509},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0162-1459},
  doi = {10.2307/2670170},
  urldate = {2025-07-09},
  abstract = {With explanatory covariates, the standard analysis for competing risks data involves modeling the cause-specific hazard functions via a proportional hazards assumption. Unfortunately, the cause-specific hazard function does not have a direct interpretation in terms of survival probabilities for the particular failure type. In recent years many clinicians have begun using the cumulative incidence function, the marginal failure probabilities for a particular cause, which is intuitively appealing and more easily explained to the nonstatistician. The cumulative incidence is especially relevant in cost-effectiveness analyses in which the survival probabilities are needed to determine treatment utility. Previously, authors have considered methods for combining estimates of the cause-specific hazard functions under the proportional hazards formulation. However, these methods do not allow the analyst to directly assess the effect of a covariate on the marginal probability function. In this article we propose a novel semiparametric proportional hazards model for the subdistribution. Using the partial likelihood principle and weighting techniques, we derive estimation and inference procedures for the finite-dimensional regression parameter under a variety of censoring scenarios. We give a uniformly consistent estimator for the predicted cumulative incidence for an individual with certain covariates; confidence intervals and bands can be obtained analytically or with an easy-to-implement simulation technique. To contrast the two approaches, we analyze a dataset from a breast cancer clinical trial under both models.}
}

@book{FlemingHarrington:2011,
  title = {Counting {{Processes}} and {{Survival Analysis}}},
  author = {Fleming, Thomas R. and Harrington, David P.},
  year = 2011,
  month = sep,
  publisher = {John Wiley \& Sons},
  abstract = {The Wiley-Interscience Paperback Series consists of selected books that have been made more accessible to consumers in an effort to increase global appeal and general circulation. With these new unabridged softcover volumes, Wiley hopes to extend the lives of these works by making them available to future generations of statisticians, mathematicians, and scientists.  "The book is a valuable completion of the literature in this field. It is written in an ambitious mathematical style and can be recommended to statisticians as well as biostatisticians." -Biometrische Zeitschrift  "Not many books manage to combine convincingly topics from probability theory over mathematical statistics to applied statistics. This is one of them. The book has other strong points to recommend it: it is written with meticulous care, in a lucid style, general results being illustrated by examples from statistical theory and practice, and a bunch of exercises serve to further elucidate and elaborate on the text." -Mathematical Reviews  "This book gives a thorough introduction to martingale and counting process methods in survival analysis thereby filling a gap in the literature." -Zentralblatt f{\"u}r Mathematik und ihre Grenzgebiete/Mathematics Abstracts  "The authors have performed a valuable service to researchers in providing this material in [a] self-contained and accessible form. . . This text [is] essential reading for the probabilist or mathematical statistician working in the area of survival analysis." -Short Book Reviews, International Statistical Institute  Counting Processes and Survival Analysis explores the martingale approach to the statistical analysis of counting processes, with an emphasis on the application of those methods to censored failure time data. This approach has proven remarkably successful in yielding results about statistical methods for many problems arising in censored data. A thorough treatment of the calculus of martingales as well as the most important applications of these methods to censored data is offered. Additionally, the book examines classical problems in asymptotic distribution theory for counting process methods and newer methods for graphical analysis and diagnostics of censored data. Exercises are included to provide practice in applying martingale methods and insight into the calculus itself.},
  isbn = {978-1-118-15066-5},
  langid = {english},
  keywords = {Mathematics / General,Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes}
}

@article{HanleyMiettinen:2009,
  title = {Fitting {{Smooth-in-Time Prognostic Risk Functions}} via {{Logistic Regression}}},
  author = {Hanley, James A. and Miettinen, Olli S.},
  year = 2009,
  month = jan,
  journal = {The International Journal of Biostatistics},
  volume = {5},
  number = {1},
  publisher = {De Gruyter},
  issn = {1557-4679},
  doi = {10.2202/1557-4679.1125},
  urldate = {2025-06-19},
  abstract = {When considering treatment options, a physician ideally has access to prognoses for various spans of prospective time, meaning known risks specific for these and also for both treatment and the profile of the patient. Accordingly, investigators ideally would report estimates of such risks from clinical trials and their non-experimental counterparts. To the extent that such risk estimates have been reported at all, they have mainly been based on the semi-parametric regression model of Cox. We focus on a family of fully-parametric hazard models of an attractive, versatile form that readily allows for non-proportionality, yet models that have not been easy to fit with standard statistical software. We elaborate an approach, recently proposed, to fitting such hazard functions via logistic regression. From the fitted hazard function, cumulative incidence and, thus, risk functions of time, treatment and profile can be derived. This approach accommodates any log-linear hazard function of prognostic time, treatment, and the prognostic indicators defining the patient's prognostic profile.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {Cox regression,logistic regression,prognosis,risk function,survival analysis},
  file = {/Users/javiermtz/Zotero/storage/JA3W27H7/Hanley and Miettinen - 2009 - Fitting Smooth-in-Time Prognostic Risk Functions via Logistic Regression.pdf}
}

@article{HouBradicXu:2019,
  title = {Inference under {{Fine-Gray}} Competing Risks Model with High-Dimensional Covariates},
  author = {Hou, Jue and Bradic, Jelena and Xu, Ronghui},
  year = 2019,
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {13},
  number = {2},
  pages = {4449--4507},
  publisher = {{Institute of Mathematical Statistics and Bernoulli Society}},
  issn = {1935-7524, 1935-7524},
  doi = {10.1214/19-EJS1562},
  urldate = {2025-07-09},
  abstract = {The purpose of this paper is to construct confidence intervals for the regression coefficients in the Fine-Gray model for competing risks data with random censoring, where the number of covariates can be larger than the sample size. Despite strong motivation from biomedical applications, a high-dimensional Fine-Gray model has attracted relatively little attention among the methodological or theoretical literature. We fill in this gap by developing confidence intervals based on a one-step bias-correction for a regularized estimation. We develop a theoretical framework for the partial likelihood, which does not have independent and identically distributed entries and therefore presents many technical challenges. We also study the approximation error from the weighting scheme under random censoring for competing risks and establish new concentration results for time-dependent processes. In addition to the theoretical results and algorithms, we present extensive numerical experiments and an application to a study of non-cancer mortality among prostate cancer patients using the linked Medicare-SEER data.},
  keywords = {62F30,62J07,62N03,high-dimensional inference,one-step estimator,p-value,Survival analysis},
  file = {/Users/javiermtz/Zotero/storage/J4MUQ7L2/Hou et al. - 2019 - Inference under Fine-Gray competing risks model with high-dimensional covariates.pdf}
}

@article{HouParavatiHou:2018,
  title = {High-Dimensional Variable Selection and Prediction under Competing Risks with Application to {{SEER-Medicare}} Linked Data},
  author = {Hou, Jiayi and Paravati, Anthony and Hou, Jue and Xu, Ronghui and Murphy, James},
  year = 2018,
  journal = {Statistics in Medicine},
  volume = {37},
  number = {24},
  pages = {3486--3502},
  issn = {1097-0258},
  doi = {10.1002/sim.7822},
  urldate = {2025-07-09},
  abstract = {Competing risk analysis considers event times due to multiple causes or of more than one event types. Commonly used regression models for such data include (1) cause-specific hazards model, which focuses on modeling one type of event while acknowledging other event types simultaneously, and (2) subdistribution hazards model, which links the covariate effects directly to the cumulative incidence function. Their use in the presence of high-dimensional predictors are largely unexplored. Motivated by an analysis using the linked SEER-Medicare database for the purposes of predicting cancer versus noncancer mortality for patients with prostate cancer, we study the accuracy of prediction and variable selection of existing machine learning methods under both models using extensive simulation experiments, including different approaches to choosing penalty parameters in each method. We then apply the optimal approaches to the analysis of the SEER-Medicare data.},
  copyright = {Copyright {\copyright} 2018 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {boosting,cumulative incidence function,electronic medical record,LASSO,machine learning,precision medicine},
  file = {/Users/javiermtz/Zotero/storage/UZZBUQBR/Hou et al. - 2018 - High-dimensional variable selection and prediction under competing risks with application to SEER-Me.pdf;/Users/javiermtz/Zotero/storage/HYVS4YFE/sim.html}
}

@incollection{HouXu:2018,
  title = {Empirical {{Study}} on {{High-Dimensional Variable Selection}} and {{Prediction Under Competing Risks}}},
  booktitle = {New {{Frontiers}} of {{Biostatistics}} and {{Bioinformatics}}},
  author = {Hou, Jiayi and Xu, Ronghui},
  editor = {Zhao, Yichuan and Chen, Ding-Geng},
  year = 2018,
  pages = {421--440},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-99389-8_21},
  urldate = {2025-07-09},
  abstract = {Competing risk analysis considers event times due to multiple causes, or of more than one event types. Commonly used regression models for such data include (1) cause-specific hazards model, which focuses on modeling one type of event while acknowledging other event types simultaneously; and (2) subdistribution hazards model, which links the covariate effects directly to the cumulative incidence function. Their use and in particular statistical properties in the presence of high-dimensional predictors are largely unexplored. We study the accuracy of prediction and variable selection of existing statistical learning methods under both models using extensive simulation experiments, including different approaches to choosing penalty parameters in each method.},
  isbn = {978-3-319-99389-8},
  langid = {english}
}

@inproceedings{JohnsonZhang:2013,
  title = {Accelerating {{Stochastic Gradient Descent}} Using {{Predictive Variance Reduction}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Johnson, Rie and Zhang, Tong},
  year = 2013,
  volume = {26},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-06-19},
  abstract = {Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this problem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we  prove that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG).  However, our analysis is significantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the storage of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.},
  file = {/Users/javiermtz/Zotero/storage/AVALPKXC/Johnson and Zhang - 2013 - Accelerating Stochastic Gradient Descent using Predictive Variance Reduction.pdf}
}

@article{KeBandyopadhyaySarkar:2023,
  title = {Gene {{Screening}} for {{Prognosis}} of {{Non-Muscle-Invasive Bladder Carcinoma}} under {{Competing Risks Endpoints}}},
  author = {Ke, Chenlu and Bandyopadhyay, Dipankar and Sarkar, Devanand},
  year = 2023,
  month = jan,
  journal = {Cancers},
  volume = {15},
  number = {2},
  pages = {379},
  issn = {2072-6694},
  doi = {10.3390/cancers15020379},
  urldate = {2025-09-11},
  abstract = {Simple Summary A vital task in contemporary cancer research is to discover clinically useful molecular markers for diagnosis and prognosis from microarray or sequencing data. However, reliable and efficient statistical tools are lacking in terms of marker screening and selection for high-throughput data with complicated survival endpoints, such as competing risks. Motivated by a study on progression of non-muscle invasive bladder carcinoma for 300 subjects with competing risk endpoints, this paper proposed a controlled screening procedure to fast eliminate most of irrelevant markers, before more precise selection can be further pursued. Combining screening with a boosting procedure, a significant six-gene signature for progression was identified subsequently, showing improved prediction performance over existing alternatives at a lower computational cost. The proposed method is readily applicable to other types of high-throughput cancer data with competing risk events, providing a desired addition to a biomedical researcher's toolbox. Abstract Background: Discovering clinically useful molecular markers for predicting the survival of patients diagnosed with non--muscle-invasive bladder cancer can provide insights into cancer dynamics and improve treatment outcomes. However, the presence of competing risks (CR) endpoints complicates the estimation and inferential framework. There is also a lack of statistical analysis tools and software for coping with the high-throughput nature of these data, in terms of marker screening and selection. Aims: To propose a gene screening procedure for proportional subdistribution hazards regression under a CR framework, and illustrate its application in using molecular profiling to predict survival for non-muscle invasive bladder carcinoma. Methods: Tumors from 300 patients diagnosed with bladder cancer were analyzed for genomic abnormalities while controlling for clinically important covariates. Genes with expression patterns that were associated with survival were identified through a screening procedure based on proportional subdistribution hazards regression. A molecular predictor of risk was constructed and examined for prediction accuracy. Results: A six-gene signature was found to be a significant predictor associated with survival of non--muscle-invasive bladder cancer, subject to competing risks after adjusting for age, gender, reevaluated WHO grade, stage and BCG/MMC treatment (p-value {$<$} 0.001). Conclusion: The proposed gene screening procedure can be used to discover molecular determinants of survival for non--muscle-invasive bladder cancer and in general facilitate high-throughput competing risks data analysis with easy implementation.},
  pmcid = {PMC9856670},
  pmid = {36672328},
  file = {/Users/javiermtz/Zotero/storage/YR97RAFY/Ke et al. - 2023 - Gene Screening for Prognosis of Non-Muscle-Invasive Bladder Carcinoma under Competing Risks Endpoint.pdf}
}

@article{KrsticMultaniWishart:2022,
  title = {The Impact of Methodological Choices When Developing Predictive Models Using Urinary Metabolite Data},
  author = {Krstic, Nikolas and Multani, Kevin and Wishart, David S. and {Blydt-Hansen}, Tom and Cohen Freue, Gabriela V.},
  year = 2022,
  journal = {Statistics in Medicine},
  volume = {41},
  number = {18},
  pages = {3511--3526},
  issn = {1097-0258},
  doi = {10.1002/sim.9431},
  urldate = {2025-06-20},
  abstract = {The continuous evolution of metabolomics over the past two decades has stimulated the search for metabolic biomarkers of many diseases. Metabolomic data measured from urinary samples can provide rich information of the biological events triggered by organ rejection in pediatric kidney transplant recipients. With additional validation, metabolic markers can be used to build clinically useful diagnostic tools. However, there are many methodological steps ranging from data processing to modeling that can influence the performance of the resulting metabolomic classifiers. In this study we focus on the comparison of various classification methods that can handle the complex structure of metabolomic data, including regularized classifiers, partial least squares discriminant analysis, and nonlinear classification models. We also examine the effectiveness of a physiological normalization technique widely used in the clinical and biochemical literature but not extensively analyzed and compared in urine metabolomic studies. While the main objective of this work is to interrogate metabolomic data of pediatric kidney transplant recipients to improve the diagnosis of T cell-mediated rejection (TCMR), we also analyze three independent datasets from other disease conditions to investigate the generalizability of our findings.},
  langid = {english},
  keywords = {machine learning,predictive modeling,sample quality,T cell-mediated rejection,urinary metabolites},
  file = {/Users/javiermtz/Zotero/storage/XFWABVNT/Krstic et al. - 2022 - The impact of methodological choices when developing predictive models using urinary metabolite data.pdf;/Users/javiermtz/Zotero/storage/28AYBZHF/sim.html}
}

@article{Li:2016,
  title = {The {{Fine}}--{{Gray}} Model under Interval Censored Competing Risks Data},
  author = {Li, Chenxi},
  year = 2016,
  month = jan,
  journal = {Journal of Multivariate Analysis},
  volume = {143},
  pages = {327--344},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2015.10.001},
  urldate = {2025-07-09},
  abstract = {We consider semiparametric analysis of competing risks data subject to mixed case interval censoring. The Fine--Gray model (Fine and Gray, 1999) is used to model the cumulative incidence function and is coupled with sieve semiparametric maximum likelihood estimation based on univariate or multivariate likelihood. The univariate likelihood of cause-specific data enables separate estimation of cumulative incidence function for each competing risk, in contrast with the multivariate likelihood of full data which estimates cumulative incidence functions for multiple competing risks jointly. Under both likelihoods and certain regularity conditions, we show that the regression parameter estimator is asymptotically normal and semiparametrically efficient, although the spline-based sieve estimator of the baseline cumulative subdistribution hazard converges at a rate slower than root-n. The proposed method is evaluated by simulation studies regarding its finite sample performance and is illustrated by a competing risk analysis of data from a dementia cohort study.},
  keywords = {Competing risk,Cumulative incidence function,Interval censored data,Semiparametric efficiency,Sieve estimation,Subdistribution hazard},
  file = {/Users/javiermtz/Zotero/storage/GVBE6J73/Li - 2016 - The Fine–Gray model under interval censored competing risks data.pdf;/Users/javiermtz/Zotero/storage/3M45H3AX/S0047259X15002481.html}
}

@book{LiXu:2009,
  title = {High-{{Dimensional Data Analysis}} in {{Cancer Research}}},
  editor = {Li, Xiaochun and Xu, Ronghui},
  year = 2009,
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-69765-9},
  urldate = {2025-07-09},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-0-387-69763-5 978-0-387-69765-9},
  langid = {english},
  keywords = {Bayesian Approaches,bioinformatics,cancer research,classification,gene expression,genes,High-Dimensional Biologic data,Laboratory,Microarray,Multivariate Nonparametric Regression,oncology,proteomics,Risk Estimation,Tree-based Methods,Vector}
}

@article{LuoRavaBradic:2025,
  title = {Doubly Robust Estimation under a Possibly Misspecified Marginal Structural {{Cox}} Model},
  author = {Luo, Jiyu and Rava, Denise and Bradic, Jelena and Xu, Ronghui},
  year = 2025,
  month = feb,
  journal = {Biometrika},
  volume = {112},
  number = {1},
  pages = {asae065},
  issn = {1464-3510},
  doi = {10.1093/biomet/asae065},
  urldate = {2025-07-09},
  abstract = {In this article we consider the marginal structural Cox model, which has been widely used to analyse observational studies with survival outcomes. The standard inverse probability weighting method under the model hinges on a propensity score model for the treatment assignment and a censoring model that incorporates both the treatment and the covariates. In such settings model misspecification can often occur, and the Cox regression model's non-collapsibility has historically posed challenges when striving to guard against model misspecification through augmentation. We introduce a novel joint augmentation to the martingale-based full-data estimating functions and develop rate double robustness, which allows the use of machine learning and nonparametric methods to overcome the challenges of non-collapsibility. We closely examine its theoretical properties to guarantee root-\$ n \$ inference for the estimand. The estimator extends naturally to estimating a time-average treatment effect when the proportional hazards assumption fails, and we show that it satisfies both the assumption-lean and the well-specification criteria in the context of a causal estimand for censoring survival data; that is, it is a functional of the potential outcome distributions only and does not depend on the treatment assignment mechanism, the covariate distribution or the censoring mechanism. The martingale-based augmentation approach is also applicable to many semiparametric failure time models. Finally, its application to a dataset provides insights into the impact of mid-life alcohol consumption on mortality in later life.},
  file = {/Users/javiermtz/Zotero/storage/9RY7IM48/asae065.html}
}

@article{Mantel:1973,
  title = {Synthetic {{Retrospective Studies}} and {{Related Topics}}},
  author = {Mantel, Nathan},
  year = 1973,
  journal = {Biometrics},
  volume = {29},
  number = {3},
  eprint = {2529171},
  eprinttype = {jstor},
  pages = {479--486},
  publisher = {International Biometric Society},
  issn = {0006-341X},
  doi = {10.2307/2529171},
  urldate = {2025-07-09},
  abstract = {Prospective and retrospective approaches for estimating the influence of several variables on the occurrence of disease are discussed. The assumptions under which these approaches would tend to yield the same estimates as would be given by an ideal but unattainable experimental design approach are stated. It is then brought out that in a large prospective study in which comparatively few cases of disease have occurred, computational problems can be so burdensome as to preclude a comprehensive and imaginative analysis of the data. The prospective study can be converted into a synthetic retrospective study by selecting a random sample of the cases and a random sample of the noncases, the sampling proportion being small for noncases, but essentially unity for cases. It is demonstrated that such sampling will tend to leave the dependence of the log odds on the variables unaffected except for an additive constant. The use of a discrimination function noniterative method of analysis is noted and is indicated to be not generally appropriate. The reverse suggestion is made that normal data can be analyzed by a log-odds approach, this yielding alternative tests to those ordinarily used for comparing two or several means or mean vectors, or two or several variances or variance-covariance matrices.}
}

@article{Mantel:1973a,
  title = {Synthetic {{Retrospective Studies}} and {{Related Topics}}},
  author = {Mantel, Nathan},
  year = 1973,
  journal = {Biometrics},
  volume = {29},
  number = {3},
  eprint = {2529171},
  eprinttype = {jstor},
  pages = {479--486},
  publisher = {International Biometric Society},
  issn = {0006-341X},
  doi = {10.2307/2529171},
  urldate = {2025-07-10},
  abstract = {Prospective and retrospective approaches for estimating the influence of several variables on the occurrence of disease are discussed. The assumptions under which these approaches would tend to yield the same estimates as would be given by an ideal but unattainable experimental design approach are stated. It is then brought out that in a large prospective study in which comparatively few cases of disease have occurred, computational problems can be so burdensome as to preclude a comprehensive and imaginative analysis of the data. The prospective study can be converted into a synthetic retrospective study by selecting a random sample of the cases and a random sample of the noncases, the sampling proportion being small for noncases, but essentially unity for cases. It is demonstrated that such sampling will tend to leave the dependence of the log odds on the variables unaffected except for an additive constant. The use of a discrimination function noniterative method of analysis is noted and is indicated to be not generally appropriate. The reverse suggestion is made that normal data can be analyzed by a log-odds approach, this yielding alternative tests to those ordinarily used for comparing two or several means or mean vectors, or two or several variances or variance-covariance matrices.}
}

@article{Miettinen:1999,
  title = {Etiologic Research: {{Needed}} Revisions of Concepts and Principles},
  shorttitle = {Etiologic Research},
  author = {Miettinen, Olli S},
  year = 1999,
  journal = {Scandinavian Journal of Work, Environment \& Health},
  volume = {25},
  number = {6},
  eprint = {40966938},
  eprinttype = {jstor},
  pages = {484--490},
  publisher = {Scandinavian Journal of Work, Environment \& Health},
  issn = {0355-3140},
  urldate = {2025-07-10},
  abstract = {Even though etiologic research has been the central concern in academic epidemiology, its concepts have remained confused or malformed, starting from that of etiology itself; and the same applies to its principles, starting from the notion that the principal variants of an etiologic study are the 'cohort' study and the 'case-control' study. This article suggests revisions of some central concepts pertaining to the object (and objective) of an etiologic study, and it posits an updated conception of the essence?singular?the study itself. This is supplemented by some novel, yet merely orientational, propositions in respect to quality-assurance in etiologic research.}
}

@article{Miettinen:2004,
  title = {Perspective: {{Epidemiology}}: {{Quo Vadis}}?},
  shorttitle = {Perspective},
  author = {Miettinen, Olli S.},
  year = 2004,
  month = aug,
  journal = {European Journal of Epidemiology},
  volume = {19},
  number = {8},
  pages = {713--718},
  issn = {1573-7284},
  doi = {10.1023/B:EJEP.0000036617.83737.74},
  urldate = {2025-07-10},
  abstract = {In our etiologic research, we epidemiologists need to leave behind the concepts of `cohort' study and `case--control' study and adopt that of the etiologic study as the singular substitute for these. We then need to realize that the etiologic study is well suited to be viewed as paradigmal for intervention studies. We finally need to become serious about object design before methods design in both etiologic and intervention research. Once these developments have occurred, we'll be ready for truly meaningful research to advance the knowledge base of both types of causality-oriented `gnosis' in the practice of clinical medicine, etiognosis and intervention-prognosis; and descriptive-prognostic study we'll see as inherent in any intervention-prognostic study. As for diagnostic research, then, we need to come to see it as nothing but a special case of our familiar descriptive prevalence research. Because of this readily attainable theoretical readiness peculiar to us research epidemiologists, and for other reasons besides, only we can assume the central role in the production of the knowledge base for scientific medicine. We consequently have the obligation to assume this larger and higher, meta-epidemiologic mission -- and some even higher ones besides.},
  langid = {english},
  keywords = {Clinical epidemiology,Critical Thinking,Ecological Epidemiology,Empiricism,Epidemiology,Ethnology,Evidence-based medicine,Medicine,Practice-as-Research,Scientific medicine,Translational Research},
  file = {/Users/javiermtz/Zotero/storage/CTTYQXK8/Miettinen - 2004 - Perspective Epidemiology Quo Vadis.pdf}
}

@article{Monterrubio-GomezConstantine-CookeVallejos:2024,
  title = {A Review on Statistical and Machine Learning Competing Risks Methods},
  author = {{Monterrubio-G{\'o}mez}, Karla and {Constantine-Cooke}, Nathan and Vallejos, Catalina A.},
  year = 2024,
  journal = {Biometrical Journal},
  volume = {66},
  number = {2},
  pages = {2300060},
  issn = {1521-4036},
  doi = {10.1002/bimj.202300060},
  urldate = {2025-07-09},
  abstract = {When modeling competing risks (CR) survival data, several techniques have been proposed in both the statistical and machine learning literature. State-of-the-art methods have extended classical approaches with more flexible assumptions that can improve predictive performance, allow high-dimensional data and missing values, among others. Despite this, modern approaches have not been widely employed in applied settings. This article aims to aid the uptake of such methods by providing a condensed compendium of CR survival methods with a unified notation and interpretation across approaches. We highlight available software and, when possible, demonstrate their usage via reproducible R vignettes. Moreover, we discuss two major concerns that can affect benchmark studies in this context: the choice of performance metrics and reproducibility.},
  copyright = {{\copyright} 2024 The Authors. Biometrical Journal published by Wiley-VCH GmbH.},
  langid = {english},
  keywords = {competing risks,risk prediction,survival analysis,time-to-event data},
  file = {/Users/javiermtz/Zotero/storage/IRXSABDE/Monterrubio-Gómez et al. - 2024 - A review on statistical and machine learning competing risks methods.pdf;/Users/javiermtz/Zotero/storage/8BLR64U3/bimj.html}
}

@article{PhamWhiteKahan:2021,
  title = {A Comparison of Methods for Analyzing a Binary Composite Endpoint with Partially Observed Components in Randomized Controlled Trials},
  author = {Pham, Tra My and White, Ian R. and Kahan, Brennan C. and Morris, Tim P. and Stanworth, Simon J. and Forbes, Gordon},
  year = 2021,
  journal = {Statistics in Medicine},
  volume = {40},
  number = {29},
  pages = {6634--6650},
  issn = {1097-0258},
  doi = {10.1002/sim.9203},
  urldate = {2025-07-17},
  abstract = {Composite endpoints are commonly used to define primary outcomes in randomized controlled trials. A participant may be classified as meeting the endpoint if they experience an event in one or several components (eg, a favorable outcome based on a composite of being alive and attaining negative culture results in trials assessing tuberculosis treatments). Partially observed components that are not missing simultaneously complicate the analysis of the composite endpoint. An intuitive strategy frequently used in practice for handling missing values in the components is to derive the values of the composite endpoint from observed components when possible, and exclude from analysis participants whose composite endpoint cannot be derived. Alternatively, complete record analysis (CRA) (excluding participants with any missing components) or multiple imputation (MI) can be used. We compare a set of methods for analyzing a composite endpoint with partially observed components mathematically and by simulation, and apply these methods in a reanalysis of a published trial (TOPPS). We show that the derived composite endpoint can be missing not at random even when the components are missing completely at random. Consequently, the treatment effect estimated from the derived endpoint is biased while CRA results without the derived endpoint are valid. Missing at random mechanisms require MI of the components. We conclude that, although superficially attractive, deriving the composite endpoint from observed components should generally be avoided. Despite the potential risk of imputation model mis-specification, MI of missing components is the preferred approach in this study setting.},
  copyright = {{\copyright} 2021 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {compatibility,composite endpoints,missing data,multiple imputation,RCTs},
  file = {/Users/javiermtz/Zotero/storage/YUCAJ8KY/Pham et al. - 2021 - A comparison of methods for analyzing a binary composite endpoint with partially observed components.pdf;/Users/javiermtz/Zotero/storage/6Q3K7LAZ/sim.html}
}

@article{PrenticeKalbfleischPeterson:1978,
  title = {The {{Analysis}} of {{Failure Times}} in the {{Presence}} of {{Competing Risks}}},
  author = {Prentice, R. L. and Kalbfleisch, J. D. and Peterson, A. V. and Flournoy, N. and Farewell, V. T. and Breslow, N. E.},
  year = 1978,
  journal = {Biometrics},
  volume = {34},
  number = {4},
  eprint = {2530374},
  eprinttype = {jstor},
  pages = {541--554},
  publisher = {[Wiley, International Biometric Society]},
  issn = {0006-341X},
  doi = {10.2307/2530374},
  urldate = {2025-07-09},
  abstract = {Distinct problems in the analysis of failure times with competing causes of failure include the estimation of treatment or exposure effects on specific failure types, the study of interrelations among failure types, and the estimation of failure rates for some causes given the removal of certain other failure types. The usual formulation of these problems is in terms of conceptual or latent failure times for each failure type. This approach is criticized on the basis of unwarranted assumptions, lack of physical interpretation and identifiability problems. An alternative approach utilizing cause-specific hazard functions for observable quantities, including time-dependent covariates, is proposed. Cause-specific hazard functions are shown to be the basic estimable quantities in the competing risks framework. A method, involving the estimation of parameters that relate time-dependent risk indicators for some causes to cause-specific hazard functions for other causes, is proposed for the study of interrelations among failure types. Further, it is argued that the problem of estimation of failure rates under the removal of certain causes is not well posed until a mechanism for cause removal is specified. Following such a specification, one will sometimes be in a position to make sensible extrapolations from available data to situations involving cause removal. A clinical program in bone marrow transplantation for leukemia provides a setting for discussion and illustration of each of these ideas. Failure due to censoring in a survivorship study leads to further discussion.}
}

@article{SaadatiBeyersmannKopp-Schneider:2018,
  title = {Prediction Accuracy and Variable Selection for Penalized Cause-Specific Hazards Models},
  author = {Saadati, Maral and Beyersmann, Jan and {Kopp-Schneider}, Annette and Benner, Axel},
  year = 2018,
  journal = {Biometrical Journal},
  volume = {60},
  number = {2},
  pages = {288--306},
  issn = {1521-4036},
  doi = {10.1002/bimj.201600242},
  urldate = {2025-07-09},
  abstract = {We consider modeling competing risks data in high dimensions using a penalized cause-specific hazards (CSHs) approach. CSHs have conceptual advantages that are useful for analyzing molecular data. First, working on hazards level can further understanding of the underlying biological mechanisms that drive transition hazards. Second, CSH models can be used to extend the multistate framework for high-dimensional data. The CSH approach is implemented by fitting separate proportional hazards models for each event type (iCS). In the high-dimensional setting, this might seem too complex and possibly prone to overfitting. Therefore, we consider an extension, namely ``linking'' the separate models by choosing penalty tuning parameters that in combination yield best prediction of the incidence of the event of interest (penCR). We investigate whether this extension is useful with respect to prediction accuracy and variable selection. The two approaches are compared to the subdistribution hazards (SDH) model, which is an established method that naturally achieves ``linking'' by working on incidence level, but loses interpretability of the covariate effects. Our simulation studies indicate that in many aspects, iCS is competitive to penCR and the SDH approach. There are some instances that speak in favor of linking the CSH models, for example, in the presence of opposing effects on the CSHs. We conclude that penalized CSH models are a viable solution for competing risks models in high dimensions. Linking the CSHs can be useful in some particular cases; however, simple models using separately penalized CSH are often justified.},
  copyright = {{\copyright} 2017 WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
  langid = {english},
  keywords = {competing risks,high-dimensional data,penalization,prediction},
  file = {/Users/javiermtz/Zotero/storage/S7H5SC2U/bimj.html}
}

@article{Saarela:2016,
  title = {A Case-Base Sampling Method for Estimating Recurrent Event Intensities},
  author = {Saarela, Olli},
  year = 2016,
  month = oct,
  journal = {Lifetime Data Analysis},
  volume = {22},
  number = {4},
  pages = {589--605},
  issn = {1572-9249},
  doi = {10.1007/s10985-015-9352-x},
  urldate = {2025-07-10},
  abstract = {Case-base sampling provides an alternative to risk set sampling based methods to estimate hazard regression models, in particular when absolute hazards are also of interest in addition to hazard ratios. The case-base sampling approach results in a likelihood expression of the logistic regression form, but instead of categorized time, such an expression is obtained through sampling of a discrete set of person-time coordinates from all follow-up data. In this paper, in the context of a time-dependent exposure such as vaccination, and a potentially recurrent adverse event outcome, we show that the resulting partial likelihood for the outcome event intensity has the asymptotic properties of a likelihood. We contrast this approach to self-matched case-base sampling, which involves only within-individual comparisons. The efficiency of the case-base methods is compared to that of standard methods through simulations, suggesting that the information loss due to sampling is minimal.},
  langid = {english},
  keywords = {Applied Probability,Applied Statistics,Bayesian Inference,Biostatistics,Case-base sampling,Conditional logistic regression,Hazard regression,Parametric Inference,Recurrent events,Self-matching,Stochastic Analysis}
}

@article{SaarelaArjas:2015,
  title = {Non-Parametric {{Bayesian Hazard Regression}} for {{Chronic Disease Risk Assessment}}},
  author = {Saarela, Olli and Arjas, Elja},
  year = 2015,
  journal = {Scandinavian Journal of Statistics},
  volume = {42},
  number = {2},
  pages = {609--626},
  issn = {1467-9469},
  doi = {10.1111/sjos.12125},
  urldate = {2025-07-11},
  abstract = {ABSTRACTAssessing the absolute risk for a future disease event in presently healthy individuals has an important role in the primary prevention of cardiovascular diseases (CVD) and other chronic conditions. In this paper, we study the use of non-parametric Bayesian hazard regression techniques and posterior predictive inferences in the risk assessment task. We generalize our previously published Bayesian multivariate monotonic regression procedure to a survival analysis setting, combined with a computationally efficient estimation procedure utilizing case--base sampling. To achieve parsimony in the model fit, we allow for multidimensional relationships within specified subsets of risk factors, determined either on a priori basis or as a part of the estimation procedure. We apply the proposed methods for 10-year CVD risk assessment in a Finnish population. {\copyright} 2014 Board of the Foundation of the Scandinavian Journal of Statistics},
  copyright = {{\copyright} 2014\,Board of the Foundation of the Scandinavian Journal of Statistics},
  langid = {english},
  keywords = {case-base sampling,disease prediction,monotonic regression,non-parametric Bayesian regression,risk assessment}
}

@article{SaarelaHanley:2015,
  title = {Case-{{Base Methods}} for {{Studying Vaccination Safety}}},
  author = {Saarela, Olli and Hanley, James A.},
  year = 2015,
  month = mar,
  journal = {Biometrics},
  volume = {71},
  number = {1},
  pages = {42--52},
  issn = {0006-341X, 1541-0420},
  doi = {10.1111/biom.12222},
  urldate = {2025-10-31},
  abstract = {Summary             Pooling of controls under nested-case control settings can produce substantial efficiency gains compared to standard time-matched analysis using the Mantel--Haenszel method or conditional logistic regression. In the context of possible adverse effects of early childhood vaccinations, we propose pooling of the information from the controls to estimate the population exposure prevalence as a parametric or nonparametric function of time, and possibly other factors. This function in turn may be used as a plug-in estimate to control for confounding in the subsequent estimation of rate ratios. We derive standard errors for the resulting two-step estimators, demonstrate through simulations the efficiency gains compared to standard matched analysis, and propose a novel graphical presentation of the vaccination and adverse event time data. We formulate the methods in the general framework of case-base sampling, which subsumes the different case-control and case-only methods.},
  copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
  langid = {english},
  file = {/Users/javiermtz/Zotero/storage/HAZL9ANH/Saarela and Hanley - 2015 - Case-Base Methods for Studying Vaccination Safety.pdf}
}

@phdthesis{Tamvada:2023,
  title = {Penalized Competing Risks Analysis Using Casebase Sampling},
  author = {Tamvada, Nirupama},
  year = 2023,
  doi = {10.14288/1.0435526},
  urldate = {2025-06-19},
  abstract = {In biomedical studies, quantifying the association of prognostic genes/mark- ers on the time-to-event is crucial for predicting a patient's risk of disease based on their specific covariate profile. Modelling competing risks is es- sential in such studies, as patients may be susc},
  langid = {english},
  school = {University of British Columbia},
  file = {/Users/javiermtz/Zotero/storage/U3M5CZXE/Tamvada - Penalized Competing Risks Analysis using casebase sampling.pdf}
}

@article{TapakSaidijamSadeghifar:2015,
  title = {Competing {{Risks Data Analysis}} with {{High-Dimensional Covariates}}: {{An Application}} in {{Bladder Cancer}}},
  shorttitle = {Competing {{Risks Data Analysis}} with {{High-Dimensional Covariates}}},
  author = {Tapak, Leili and Saidijam, Massoud and Sadeghifar, Majid and Poorolajal, Jalal and Mahjub, Hossein},
  year = 2015,
  month = jun,
  journal = {Genomics, Proteomics \& Bioinformatics},
  volume = {13},
  number = {3},
  pages = {169--176},
  issn = {1672-0229},
  doi = {10.1016/j.gpb.2015.04.001},
  urldate = {2025-07-18},
  abstract = {Analysis of microarray data is associated with the methodological problems of high dimension and small sample size. Various methods have been used for variable selection in high-dimension and small sample size cases with a single survival endpoint. However, little effort has been directed toward addressing competing risks where there is more than one failure risks. This study compared three typical variable selection techniques including Lasso, elastic net, and likelihood-based boosting for high-dimensional time-to-event data with competing risks. The performance of these methods was evaluated via a simulation study by analyzing a real dataset related to bladder cancer patients using time-dependent receiver operator characteristic (ROC) curve and bootstrap .632+ prediction error curves. The elastic net penalization method was shown to outperform Lasso and boosting. Based on the elastic net, 33 genes out of 1381 genes related to bladder cancer were selected. By fitting to the Fine and Gray model, eight genes were highly significant (P \,\&lt;\,0.001). Among them, expression of RTN4, SON, IGF1R, SNRPE, PTGR1, PLEK, and ETFDH was associated with a decrease in survival time, whereas SMARCAD1 expression was associated with an increase in survival time. This study indicates that the elastic net has a higher capacity than the Lasso and boosting for the prediction of survival time in bladder cancer patients. Moreover, genes selected by all methods improved the predictive power of the model based on only clinical variables, indicating the value of information contained in the microarray features.}
}

@article{Van_De_Geer:2014,
  title = {Weakly Decomposable Regularization Penalties and Structured Sparsity},
  author = {Van De Geer, Sara},
  year = 2014,
  month = mar,
  journal = {Scandinavian Journal of Statistics},
  volume = {41},
  number = {1},
  pages = {72--86},
  issn = {0303-6898, 1467-9469},
  doi = {10.1111/sjos.12032},
  urldate = {2025-10-31},
  abstract = {ABSTRACT                            It has been shown in literature that the Lasso estimator, or               {$\ell$}               1               -penalized least squares estimator, enjoys good oracle properties. This paper examines which special properties of the               {$\ell$}               1               -penalty allow for sharp oracle results, and then extends the situation to general norm-based penalties that satisfy a weak decomposability condition.},
  langid = {english},
  file = {/Users/javiermtz/Zotero/storage/PLCDV32U/Van De Geer - 2014 - Weakly decomposable regularization penalties and structured sparsity.pdf}
}

@article{Van_De_GeerBuhlmannRitov:2014,
  title = {On Asymptotically Optimal Confidence Regions and Tests for High-Dimensional Models},
  author = {Van De Geer, Sara and B{\"u}hlmann, Peter and Ritov, Ya'acov and Dezeure, Ruben},
  year = 2014,
  month = jun,
  journal = {The Annals of Statistics},
  volume = {42},
  number = {3},
  issn = {0090-5364},
  doi = {10.1214/14-AOS1221},
  urldate = {2025-10-31},
  file = {/Users/javiermtz/Zotero/storage/LGI68UPM/Van De Geer et al. - 2014 - On asymptotically optimal confidence regions and tests for high-dimensional models.pdf}
}

@article{WangShenTan:2022,
  title = {Fast {{Lasso}}-type Safe Screening for {{Fine}}-{{Gray}} Competing Risks Model with Ultrahigh Dimensional Covariates},
  author = {Wang, Hong and Shen, Zhenyuan and Tan, Zhelun and Zhang, Zhuan and Li, Gang},
  year = 2022,
  month = oct,
  journal = {Statistics in Medicine},
  volume = {41},
  number = {24},
  pages = {4941--4960},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/sim.9545},
  urldate = {2025-09-02},
  abstract = {The Fine-Gray proportional sub-distribution hazards (PSH) model is among the most popular regression model for competing risks time-to-event data. This article develops a fast safe feature elimination method, named PSH-SAFE, for fitting the penalized Fine-Gray PSH model with a Lasso (or adaptive Lasso) penalty. Our PSH-SAFE procedure is straightforward to implement, fast, and scales well to ultrahigh dimensional data. We also show that as a feature screening procedure, PSH-SAFE is safe in a sense that the eliminated features are guaranteed to be inactive features in the original Lasso (or adaptive Lasso) estimator for the penalized PSH model. We evaluate the performance of the PSH-SAFE procedure in terms of computational efficiency, screening efficiency and safety, run-time, and prediction accuracy on multiple simulated datasets and a real bladder cancer data. Our empirical results show that the PSH-SAFE procedure possesses desirable screening efficiency and safety properties and can offer substantially improved computational efficiency as well as similar or better prediction performance in comparison to their baseline competitors.},
  langid = {english},
  file = {/Users/javiermtz/Zotero/storage/QI7A97E6/Wang et al. - 2022 - Fast Lasso‐type safe screening for Fine‐Gray competing risks model with ultrahigh dimensional covari.pdf}
}
