---
title: "Opt performance"
# author: Javier Mtz.-Rdz.
date-format: long
number-sections: true
number-depth: 4
fig-dpi: 400
format: 
  pdf:
    documentclass: article
    header-includes: |
      \usepackage[left=0.7in,right=0.7in,
      top=0.7in,bottom=0.5in,footskip=0.7in]{geometry} 
      \usepackage[document]{ragged2e}
      \usepackage{amsmath,amsthm,amssymb,amsfonts}
      \usepackage{mathtools}
      % Using kp fonts
      \usepackage{kpfonts}
      \usepackage{dsfont}
      \usepackage{centernot}
      \usepackage[usenames,dvipsnames,table]{xcolor}
      \usepackage{booktabs} % For improved table lines
      \renewcommand{\arraystretch}{1} % Increase row spacing
      \renewcommand\thefigure{\arabic{figure}}
    fontsize: 12pt
    colorlinks: true
knitr:
  opts_chunk:
    comment: "#>"
    message: FALSE
    dev: "png"
    fig.width: 8
    fig.height: 4.5
    fig.align: center
editor_options: 
  chunk_output_type: console
---

```{r preprocessing, include=FALSE}
# Setup ----
## Packages to use ----

#' To install mytidyfunctions, you need 
#' remotes::install_github("JavierMtzRdz/mytidyfunctions")
if (!require("pacman")) install.packages("pacman")
if (!require("mytidyfunctions")) remotes::install_github("JavierMtzRdz/mytidyfunctions")


pacman::p_load(tidyverse, janitor, writexl, 
              readxl, scales, mytidyfunctions,
              patchwork, here, 
              mtool, bench)

library(casebase)
library(future.apply)
library(glmnet)
library(mtool)
library(parallel)
library(tictoc)
library(tidyverse)
library(foreach)
library(survival)
library(cmprsk)
library(glue)
library(pec)
library(survminer)

## Load fonts ----
extrafont::loadfonts(quiet = TRUE)

## Set theme ------
mytidyfunctions::set_mytheme(text = element_text(family = "Times New Roman"))


# Set-up process

save <- F

```



This documents summarizes the main observations about the performance of the implementation of the Regularized Multinomial Regression in `mtool`. This implementation uses a Stochastic Variance Reduced Gradient (SVRG) algorithm in order to solve the proximal operator. 

# Generative Data Model ------

Data are simulated from a $K=2$ competing risks proportional hazards model. The cause-specific hazard for cause $k \in \{1, 2\}$ for individual $i$ with covariates $X_i$ is $\lambda_k(t | X_i) = \lambda_{0k}(t) \exp(X_i^T \beta_k)$. The baseline hazards $\lambda_{0k}(t)$ follow a Weibull distribution $\lambda_{0k}(t) = h_k \gamma_k t^{\gamma_k - 1}$, with parameters $(h_1, \gamma_1)=(0.55, 1.5)$ and $(h_2, \gamma_2)=(0.05, 1.5)$, implying increasing baseline hazards, higher for cause 1. 

The coefficient vectors $\beta_1, \beta_2 \in \mathbb{R}^p$ are sparse. Only the first 10 covariates $X_1, \dots, X_{10}$ have non-zero effects: $\beta_{1j} = +1$ and $\beta_{2j} = -1$ for $j=1,\dots,10$, with all other $\beta_{kj}=0$. These covariates increase $\lambda_1(t|X_i)$ and decrease $\lambda_2(t|X_i)$. 

Event times $T_i$ and causes $C_i$ are generated by simulating potential failure times $T_{ik}$ from $\lambda_k(t|X_i)$ and setting $T_i = \min(T_{i1}, T_{i2})$ with $C_i$ being the index yielding the minimum. Independent censoring times $T_{cens, i}$ (rate 0.25) are generated. Observed data consist of $(ftime_i, fstatus_i)$, where $ftime_i = \min(T_i, T_{cens, i})$ and the status $fstatus_i = C_i \cdot \mathds{1}(T_i \le T_{cens, i})$ (with $fstatus_i=0$ indicating censoring).

```{r}
###############################################
#' Create the case-base sampled dataset
#'
#' @param surv_obj
#' @param cov_matrix
#' @param ratio
#' @return List of 4 elements, containing the necessary data to fit a case-base
#'   regression model.
create_cbDataset <- function(surv_obj, cov_matrix, ratio = 5) {
  n <- nrow(surv_obj)
  B <- sum(surv_obj[, "time"])
  c1 <- sum(surv_obj[, "status"] == 1)
  c2 <- sum(surv_obj[, "status"] == 2)
  b <- ratio * (c1)
  offset <- log(B/b)
  prob_select <-  surv_obj[, "time"]/B
  # Create base series
  which_pm <- sample(n, b, replace = TRUE, prob = prob_select)
  bSeries <- as.matrix(surv_obj[which_pm, ])
  time_bseries <- runif(b) * bSeries[, "time"]
  cov_bseries <- cov_matrix[which_pm, , drop = FALSE]
  event_bseries <- rep(0L, nrow(bSeries))
  # Extract case series
  cSeries <- as.matrix(surv_obj[surv_obj[,"status"] != 0L, ])
  time_cseries <- cSeries[,"time"]
  cov_cseries <- cov_matrix[surv_obj[,"status"] != 0L, , drop = FALSE]
  event_cseries <- cSeries[,"status"]
  # Combine and return
  output <- list("time" = c(time_bseries, time_cseries),
                 "event_ind" = c(event_bseries, event_cseries),
                 "covariates" = rbind(cov_bseries, cov_cseries),
                 "offset" = rep(offset, nrow(bSeries) + nrow(cSeries)))
  
  return(output)
}

########### Function to compute weibull hazard ################################
weibull_hazard <- Vectorize(function(gamma, lambda, t) {
  return(gamma * lambda * t^(gamma - 1))
})

############ Function to simulate from cause-specific hazards #################
cause_hazards_sim <- function(p, n, beta1, beta2, 
                              nblocks = 4, cor_vals = c(0.7, 0.4, 0.6, 0.5), 
                              num.true = 20, h1 = 0.55, h2 = 0.10, 
                              gamma1 = 100, gamma2 = 100, max_time = 1.5,
                              noise_cor = 0.1, 
                              rate_cens = 0.05, min_time = 0.002,
                              exchangeable = FALSE) {
  # Warnings
  if(length(beta1) != length(beta2)) stop("Dimension of beta1 and beta2 should be the same")
  if(nblocks != length(cor_vals)) stop("Dim of nblocks and corr for blocks should match")
  if(isTRUE(exchangeable)) {
    # Create an empty matrix
    mat <- matrix(noise_cor, nrow = p, ncol = p)
    # Set the correlation values
    cor_exchangeable <- 0.5
    # Set the upper triangular and lower triangular parts
    mat[1:num.true, 1:num.true] <- cor_exchangeable
    # Print the matrix
    diag(mat) <- rep(1, length(diag(mat)))
    X <- mvtnorm::rmvnorm(n, mean = rep(0, p), sigma = mat)
  } else {
    # Set the number of variables per block
    vpb <- num.true/nblocks
  # Set the correlation values for each covariate block
  correlation_values <- cor_vals
  # Initialize empty matrix
  correlation_matrix <- matrix(noise_cor, nrow = p, ncol = p)
  # Generate the covariance matrix with block correlations
  for (i in 1:nblocks) {
    start_index <- (i - 1) * vpb + 1
    end_index <- i * vpb
    correlation_matrix[start_index:end_index, start_index:end_index] <- correlation_values[i]
  }
  # Diagonal elements should be 1
  diag(correlation_matrix) <- rep(1, length(diag(correlation_matrix)))
  X <- mvtnorm::rmvnorm(n, mean = rep(0, p), sigma = correlation_matrix)
  }
  X <- as.matrix(X)
  # Specify rate parameters
  lambda1 <- h1 * exp(X %*% beta1)
  lambda2 <- h2 * exp(X %*% beta2)
  # Define cdf - U 
  cdf_U <- function(t, gamma1, lambda1, gamma2, lambda2, U) {
    F_min_U <- 1 - exp(-(lambda1 * t^gamma1 + lambda2 * t^gamma2)) - U
    return(F_min_U)
  }
  # Generate uniform values and store in dataframe
  u <- stats::runif(n)
  # Inverse transform sampling 
  dat_roots <- cbind.data.frame(u, gamma1, lambda1, gamma2, lambda2)
  times <- dat_roots %>%
    dplyr::rowwise() %>%
    dplyr::mutate(
      t_tilde = stats::uniroot(
        cdf_U,
        interval = c(.Machine$double.eps,
                     max_time),
        extendInt = "yes",
        U = u,
        gamma1 = gamma1,
        lambda1 = lambda1,
        gamma2 = gamma2,
        lambda2 = lambda2
      )$`root`
    ) %>%
    dplyr::pull(t_tilde)
  # Generate event indicators
  hazard1 <- weibull_hazard(gamma= gamma1, lambda = lambda1, t = times)
  hazard2 <- weibull_hazard(gamma = gamma2, lambda = lambda2, t = times)
  event <- stats::rbinom(n = n, size = 1, prob = hazard1 / (hazard1 + hazard2))
  c.ind <- ifelse(event == 1, 1, 2)
  # Add censoring
  cens <- stats::rexp(n = n, rate = rate_cens)
  c.ind <- ifelse(cens < times, 0, c.ind)
  times <- pmin(cens, times)
  # Winsorize time ranges to desired ones to make them more realistic 
  # and add some white noise 
  c.ind <- ifelse(times >= max_time, 0, c.ind)
  times <- ifelse(times >= max_time, max_time, times)
  times[times == max_time] <- times[times == max_time] +
      rnorm(length(times[times == max_time]), mean = 0, sd = 1e-4)
  times <- ifelse(times < min_time, min_time, times)
  times[times == min_time] <- times[times == min_time] +
      abs(rnorm(length(times[times == min_time]), mean = 0, sd = 1e-4))
  sim.data <- data.frame(fstatus = c.ind, ftime = times)
  X <- as.data.frame(X)
  colnames(X) <- paste0("X", seq_len(p))
  sim.data <- as.data.frame(cbind(sim.data, X))
  return(sim.data)
}

  gen_data <- function(n, p) {
        num_true <- 20
        beta1 <- c(rep(0, p))
        beta2 <- c(rep(0, p))
        nu_ind <- seq(num_true)
        # Here out of 20 predictors, 10 should be non-zero 
        beta1[nu_ind] <- c(rep(1, 10), rep(0, 10))
        beta2[nu_ind] <- c(rep(-1, 10), rep(0, 10))
        
        # Simulate data
        sim.data <- cause_hazards_sim(n = n, p = p, 
                                      beta1 = beta1, beta2 = beta2,
                                      rate_cens = 0.25, 
                                      h1 = 0.55, h2 = 0.05,
                                      gamma1 = 1.5, gamma2 = 1.5, 
                                      exchangeable = TRUE)
        
        
        cen.prop <- c(prop.table(table(sim.data$fstatus)), 0, 0, 0, 0)
        
        # Training-test split 
        # We only do this (instead of generating datasets for train and test 
        # like Anthony mentioned because it is faster computationally 
        # as casebase resamples) + proportion of censoring can be quite random 
        # in each run of the simulation so we want to maintain the same in 
        # validation and test set
        
        train.index <- caret::createDataPartition(sim.data$fstatus, p = 0.75, list = FALSE)
        train <- sim.data[train.index,]
        test <- sim.data[-train.index,]
        
        ##############################################################
        # We have two competitor models for variable selection:
        # 1) Independent cox-regression model 
        # 2) penCR cox regression model - where the lambda penalties are trained together 
        ######################## Fit indepedent cox-regression model ###############################
        ######################### Cause-1 #########################################
        # Censor competing event
        y_train <- Surv(time = train$ftime, event = train$fstatus == 1)
        
        x_train <- model.matrix(~ . -ftime -fstatus, data = train)[, -1] 
        
        # Censor competing event
        y_test <- Surv(time = test$ftime, event = test$fstatus == 1)
        
        x_test <- model.matrix(~ . -ftime -fstatus, data = test)[, -1] 
        
        # Test set 
        surv_obj_val <- with(test, Surv(ftime, as.numeric(fstatus), type = "mstate"))
        
        # Covariance matrix
        cov_val <- cbind(test[, c(grepl("X", colnames(test)))], time = log(test$ftime))
        
        # Case-base dataset
        cb_data_val <- create_cbDataset(surv_obj_val, as.matrix(cov_val), ratio = 10)
        
        
        return(list(X_train = cb_data_val$covariates,
                    Y_train = cb_data_val$event_ind,
                    offset = cb_data_val$offset,
                    true_beta = cbind(beta1, beta2)))
    }

```


# Performance Evaluation ------

In order to evaluate the performance of `MNlogistic`, we assess its convergence time and memory allocated with sample sizes of $n = 1000, 2000$ and covariates $p = 20, 100, 250, 500, 1000, 2000, 4000$. The original implementation is compared with an improved implementation `MNlogistic2` with the following changes:

* Matrix input `U` in `proximal` functions (Flat, Graph, Tree) are directly called using `U.memptr()`
* String `regul` in `proximal` functions (Flat, Graph, Tree) are directly called using `std::vector<char>`.
* Multinomial gradient functions changed to in-place calculation (takes `arma::mat& grad_out`, returns `void`), removing internal allocation.
* Multinomial gradient functions optimized to create column vector from x only once per call.


```{r}
if (save){
    results <- bench::press(
        n = c(1000, 2000),
        p = c(20, 100, 250, 500, 1000, 2000, 4000),
        lambda = c(0.9, 0.5, 0.07, 0.005),
        {
            data <- gen_data(n, p)
            # lambda <- 0.9
            alpha = 0.5
            unpen_cov = 2
            # Elastic-net reparametrization
            lambda1 <- lambda*alpha
            lambda2 <- 0.5*lambda*(1 - alpha)
            # Prepare covariate matrix with intercept
            set.seed(1234)
            bench::mark(
                MNlogistic = mtool::mtool.MNlogistic(
                    X = data$X_train,
                    Y = data$Y_train,
                    offset = data$offset,
                    N_covariates = 2,
                    regularization = 'elastic-net',
                    transpose = FALSE,
                    lambda1 = lambda1, lambda2 = lambda2, 
                    lambda3 = 0
                ),
                   MNlogistic2 = mtool::mtool.MNlogistic2(
                    X = data$X_train,
                    Y = data$Y_train,
                    offset = data$offset,
                    N_covariates = 2,
                    regularization = 'elastic-net',
                    transpose = FALSE,
                    lambda1 = lambda1, lambda2 = lambda2, 
                    lambda3 = 0
                ),
                       MNlogisticPCD = mtool::mtool.MNlogisticPCD(
                    X = data$X_train,
                    Y = data$Y_train,
                    offset = data$offset,
                    N_covariates = 2,
                    regularization = 'elastic-net',
                    transpose = FALSE,
                    lambda1 = lambda1, lambda2 = lambda2, 
                    lambda3 = 0
                ),
                glmnet = glmnet(data$X_train,
                                data$Y_train,
                                alpha = alpha, 
                                lambda = lambda,
                                family = "multinomial"),
                min_iterations = 1, check = FALSE
            )
        }
    )
    
    pushoverr::pushover(message = "Loop finished", 
                        user = "uk2t3aqv1tavuxiuuzag7pf7742q15", 
                        app = "aq5nckww93an2onvb4k1jin9487gak")
                        
    
    saveRDS(results, here::here( "notes_jmr", "data", "results.rds"))
}
```

## Plot results

### Time
```{r}
results <- readRDS(here::here("notes_jmr", "data", "results.rds"))

results %>% 
    as.tibble() %>% 
    unnest_longer(time) %>% 
    ggplot(aes(x = p, y = time, group = paste0(expression, p, n),
               colour = as.character(expression))) +
    geom_jitter(alpha = 0.1) +
    stat_summary(geom = "point", fun.y = "mean", alpha = 1) +
    stat_summary(fun.data = mean_sdl, geom = "errorbar", width = 0.1) +
    stat_summary(aes(group = paste0(expression, n)), fun = mean, geom = "line") +
    facet_wrap(~paste0("n = ", n)) +
    labs(x = "p",
         y = "Time",
         colour = "Implementation")
```

### Memory
```{r}

results %>% 
    as.tibble() %>% 
    ggplot(aes(x = p, y = mem_alloc, group = paste0(expression, p, n),
               colour = as.character(expression))) +
    geom_jitter(alpha = 0.1) +
    stat_summary(geom = "point", fun.y = "mean", alpha = 1) +
    stat_summary(fun.data = mean_sdl, geom = "errorbar", width = 0.1) +
    stat_summary(aes(group = paste0(expression, n)), fun = mean, geom = "line") +
    facet_wrap(~paste0("n = ", n)) +
    labs(x = "p",
          y = "Memory allocated",
         colour = "Implementation")

```

## Optimization Example

```{r}
# Objetive function

objective <- function(B, X, Y, offset,
                      lambda, alpha, reg_p) {
    
    
    N_prime <- nrow(X)
    p <- ncol(X)
    K <- ncol(B)
    Y_int <- as.integer(Y)
    Y <- Y_int
    valid_y_indices <- which(Y >= 1 & Y <= K)
    if (length(valid_y_indices) < N_prime) {
        if (length(valid_y_indices) == 0) {
            loss_value <- 0.0
            
            penalty_value <- 0.0
            if (reg_p > 0 && lambda > 0) {
                B_reg <- B[1:reg_p, , drop = FALSE]
                lambda1 <- lambda * alpha
                lambda2 <- 0.5 * lambda * (1 - alpha)
                penalty_value <- (lambda1 * sum(abs(B_reg))) + (lambda2 * sum(B_reg^2))
            }
            return(penalty_value)
        }

        
        X_filt <- X[valid_y_indices, , drop = FALSE]
        Y_filt <- Y[valid_y_indices]
        offset_filt <- offset[valid_y_indices]
        N_filt <- length(valid_y_indices)
    } else {
        # Use all data if all Y are valid
        X_filt <- X
        Y_filt <- Y
        offset_filt <- offset
        N_filt <- N_prime
    }
    
    # eta = X %*% B + offset (N_filt x K)
    eta <- sweep(X_filt %*% B, 1, offset_filt, "+")
    
    # log(1 + sum_k(exp(eta_ik))) for each row i
    max_eta_stable <- apply(cbind(0, eta), 1, max) # max of 0 and row max(eta_k)
    log_one_plus_sum_exp <- max_eta_stable + log( exp(-max_eta_stable) + rowSums(exp(sweep(eta, 1, max_eta_stable, "-"))) )
    
    idx_mat <- cbind(seq_len(N_filt), Y_filt) 
    eta_yi <- eta[idx_mat] 
    
    #  -eta_iyi + log(1 + sum_k exp(eta_ik))
    loss_per_obs <- -eta_yi + log_one_plus_sum_exp
    
    loss_value <- sum(loss_per_obs)
    
    # Penalty
    penalty_value <- 0.0
    if (reg_p > 0 && lambda > 0) {
        
        B_reg <- B[1:reg_p, , drop = FALSE]
        
        lambda1 <- lambda * alpha
        lambda2 <- 0.5 * lambda * (1 - alpha)
        
        l1_norm <- sum(abs(B_reg))
        
        l2_norm_sq <- sum(B_reg^2)
        
        penalty_value <- (lambda1 * l1_norm) + (lambda2 * l2_norm_sq)
    }
    
    return(loss_value + penalty_value)
}


objective <- function(B, X, Y, offset,
                      lambda, alpha,
                      reg_p = NULL,
                      weights = NULL,
                      epsilon = 1e-15) {

  # --- Input Checks ---
  if (!is.matrix(B)) stop("B must be a matrix.")
  if (!is.matrix(X)) stop("X must be a matrix.")
  if (!is.numeric(Y)) stop("Y must be a numeric vector.") # Allow numeric
  if (!is.numeric(offset)) stop("offset must be a numeric vector.") # Allow numeric

  n_obs <- nrow(X)
  p_fit <- ncol(X) # Number of features in X, including intercept
  K_event_classes <- ncol(B) # Number of non-reference event classes

  if (is.null(weights)) {
    weights <- rep(1.0, n_obs)
  } 
  Y_int <- as.integer(Y)
  if (!isTRUE(all.equal(Y, Y_int))) warning("Y converted to integer.")
  Y <- Y_int
  unique_Y_values <- unique(Y)
  if (!all(unique_Y_values %in% 0:K_event_classes)) {
      warning("Y contains values outside the expected range [0, K_event_classes].")
  }


  # --- 1. Calculate Loss Term: Negative Log-Likelihood ---
  # Linear predictors: eta_ik = X_i %*% beta_k + offset_i
  # eta will be n_obs x K_event_classes
  eta <- X %*% B
  eta <- sweep(eta, 1, offset, "+") # Add offset to each column of eta

  # Calculate denominator for probabilities: 1 + sum_k(exp(eta_ik)) for each observation i
  # Use numerically stable log-sum-exp trick for log(1 + sum_k exp(eta_ik))
  # log_denom_i = max_stable_i + log(exp(-max_stable_i) + sum_k exp(eta_ik - max_stable_i))
  # where max_stable_i = max(0, max_k(eta_ik)) to include the "1" (which is exp(0)).

  log_denominators <- numeric(n_obs)
  for (i in 1:n_obs) {
      eta_i_row <- eta[i, , drop = TRUE]
      max_val_for_row <- max(0, eta_i_row) # max(0, max_k(eta_ik))
      log_denominators[i] <- max_val_for_row +
                           log(exp(-max_val_for_row) + sum(exp(eta_i_row - max_val_for_row)))
  }
  # This log_denominators[i] is log(1 + sum_l exp(eta_il))

  # Calculate log-likelihood contributions
  neg_log_lik_terms <- numeric(n_obs)
  for (i in 1:n_obs) {
    if (Y[i] == 0) { # Reference class
      neg_log_lik_terms[i] <- log_denominators[i] # -log(P(Y_i=0)) = -log(1/Denom) = log(Denom)
    } else if (Y[i] >= 1 && Y[i] <= K_event_classes) { # Event class j = Y[i]
      # -log(P(Y_i=j)) = -log(exp(eta_ij)/Denom) = -(eta_ij - log(Denom)) = -eta_ij + log(Denom)
      neg_log_lik_terms[i] <- -eta[i, Y[i]] + log_denominators[i]
    } else {
      # Handle unexpected Y values if necessary, or assume they are filtered out
      neg_log_lik_terms[i] <- 0 # Or NA, or skip
    }
  }

  # Total weighted negative log-likelihood
  loss_value <- sum(weights * neg_log_lik_terms)


  # --- 2. Calculate Penalty Term P(B) ---
  penalty_value <- 0.0
  if (lambda > 0) {
    if (is.null(p_fit)) {
      # If no reg_p, assume all p_fit predictors in B are penalized equally (factor 1)
      # EXCEPT if B also includes an intercept that shouldn't be penalized by default.
      # The paper sets w_k=0 for intercept and time.
      # User must supply correct reg_p. For now, assume it means apply to all B.
      warning("reg_p not supplied. Applying penalty to all coefficients in B. Specify reg_p=0 for unpenalized coefficients.")
      B_reg <- B # Penalize all of B
    } else {
      # Create a matrix of penalty factors replicated for K_event_classes columns
      # This allows element-wise multiplication if desired, or just use reg_p for row-wise norms
      # The paper's formula lambda * sum_{s} w_s ( L2_row_s + L1_row_s )
      # means we apply w_s to the sum of penalties for row s.

      # We need to sum w_s * L1_norm_of_row_s and w_s * L2_norm_sq_of_row_s
      # This implementation applies penalty to the whole B_reg, so reg_p
      # effectively decide which rows constitute B_reg.
      # Let's interpret reg_p as selecting which rows are in B_reg
      # This means if reg_p[s] == 0, row s is NOT in B_reg.
      # This is equivalent to reg_p logic if reg_p are 1 for first reg_p, then 0.
      
      # For simplicity, this R function will use reg_p logic if reg_p is just a scalar reg_p.
      # If reg_p is a vector, it means w_k from paper.
      # Let's assume for this function, like the C++, reg_p defines the block.
      # The user of this R function would prepare B accordingly or this function needs reg_p.
      # For now, I'll assume the passed B already has unpenalized rows set to zero if needed,
      # or reg_p does the selection.
      # The prompt's previous C++ code used `reg_p`.
      # Equation 3.8 defines w_k, which is more flexible.
      # Let's stick to the paper's Eq 3.8 with reg_p as w_k.
      
        reg_p <- rep(1, p_fit) # Default: penalize all rows
      
      # Calculate L1 and L2 components for each row, then weight by reg_p
      l1_component_sum <- 0
      l2_component_sum <- 0
      # browser()
      for (s in 1:p_fit) {
          if (reg_p[s] > 0) { # Only if w_s > 0
              l1_component_sum <- l1_component_sum + reg_p[s] * sum(abs(B[s,]))
              l2_component_sum <- l2_component_sum + reg_p[s] * sum(B[s,]^2)
          }
      }
      lambda_alpha_L1 <- lambda * alpha
      lambda_alpha_L2 <- lambda * (1 - alpha) / 2.0 # Matches (1-alpha)/2 * beta_kj^2

      penalty_value <- (lambda_alpha_L1 * l1_component_sum) + (lambda_alpha_L2 * l2_component_sum)
    }
  }

  # --- 3. Return Total Objective ---
  return(loss_value + penalty_value)
}
```

```{r}
if (save){
set.seed(123)

iter_sim <- tibble()

for (i in 1) {
n <- 1000
p <- 200
lambda = c(0.1)
data <- gen_data(n, p)
alpha = 0.5
unpen_cov = 2
# Elastic-net reparametrization
lambda1 <- lambda*alpha
lambda2 <- 0.5*lambda*(1 - alpha)

MNlogistic3 <-  mtool::mtool.MNlogistic3(
    X = data$X_train,
    Y = data$Y_train,
    offset = data$offset,
    N_covariates = unpen_cov,
    regularization = 'elastic-net',
    transpose = FALSE,
    lambda1 = lambda1, lambda2 = lambda2, 
    lambda3 = 0,
    learning_rate = 1e-1,
    # niter_inner_mtplyr = 2,
    maxit = 2000,
    tolerance = 1e-10
)

MNlogisticPCD <-  mtool::mtool.MNlogisticCCD(
    X = data$X_train,
    Y = data$Y_train,
    offset = data$offset,
    N_covariates = unpen_cov,
    regularization = 'elastic-net',
    transpose = FALSE,
    lambda1 = lambda1, lambda2 = lambda2, 
    lambda3 = 0,
    learning_rate = 5,
    # niter_inner_mtplyr = 2,
    maxit = 2000,
    tolerance = 1e-6
)

MNlogistic3$coefficients
MNlogisticPCD$coefficients

iter_vals <- map_dbl(MNlogisticPCD$coefficientshist, ~
                         objective(matrix(.x, 
                                          nrow = ncol(data$X_train)), 
                                   data$X_train, data$Y_train, data$offset,
                                   lambda, alpha, 20))

   if (i == 1) iter_sim <- tibble()

iter_sim <- tibble(obj_step = iter_vals, 
       iter = 1:length(iter_vals),
       sim = i) %>% 
bind_rows(iter_sim)
}

saveRDS(iter_sim, here::here( "notes_jmr", "data", "opt-example.rds"))

}

iter_sim <- readRDS(here::here("notes_jmr", "data", "opt-example.rds"))

iter_sim %>% 
    ggplot(aes(iter, y = obj_step, group = as.character(sim))) +
    geom_line(alpha = 0.8) +
    # geom_smooth(aes(group = 1)) +
    labs(y = "Obj. funct",
         x = "Iteration")

```


### Comparison

```{r}
if (save){

for (i in 1) {
    i <- 1
    seed <- as.integer(i)
    
    # take the last five digits of the initial seed
    the_seed = seed %% 100000
    
    set.seed(the_seed)
    n <- 1000
    p <- 2000
    lambda = c(.1)
    data <- gen_data(n, p)
    alpha = 0.5
    unpen_cov = 2
    # Elastic-net reparametrization
    lambda1 <- lambda*alpha
    lambda2 <- 0.5*lambda*(1 - alpha)
    
    
    set.seed(the_seed)
    time <- system.time({MNlogistic <-  mtool::mtool.MNlogistic2(
        X = data$X_train,
        Y = data$Y_train,
        offset = data$offset,
        N_covariates = unpen_cov,
        regularization = 'elastic-net',
        transpose = FALSE,
        lambda1 = lambda1, lambda2 = lambda2, 
        lambda3 = 0,
        niter_inner_mtplyr = 7, maxit = 100,
        tolerance = 1e-4
    )})
    
    set.seed(the_seed)
    timeExp <- system.time({MNlogisticExp <- mtool::mtool.MNlogisticExp(
        X = data$X_train,
        Y = data$Y_train,
        offset = data$offset,
        N_covariates = unpen_cov,
        regularization = 'elastic-net',
        transpose = FALSE,
        lambda1 = lambda1, lambda2 = lambda2, 
        lambda3 = 0,
        niter_inner_mtplyr = 2, maxit = 100,
        tolerance = 1e-4,
        learning_rate = 1e-3
    )})
    
    set.seed(the_seed)
    timeAcc <- system.time({MNlogisticAcc <- mtool::mtool.MNlogisticAcc(
        X = data$X_train,
        Y = data$Y_train,
        offset = data$offset,
        N_covariates = unpen_cov,
        regularization = 'elastic-net',
        transpose = FALSE,
        lambda1 = lambda1, lambda2 = lambda2, 
        lambda3 = 0,
        niter_inner_mtplyr = 1,
        maxit = 50,
        momentum_gamma = .9,
        tolerance = 1e-1,
        learning_rate = 1e-4,
    )})
    
    set.seed(the_seed)
    timeSAHRA <- system.time({MNlogisticSAHRA <- mtool::mtool.MNlogisticSAHRA(
        X = data$X_train,
        Y = data$Y_train,
        offset = data$offset,
        N_covariates = unpen_cov,
        regularization = 'elastic-net',
        transpose = F,
        lambda1 = lambda1, lambda2 = lambda2, 
        lambda3 = 0,
        niter_inner_mtplyr = 1,
        maxit = 50,
        tolerance = 1e-4,
        learning_rate = 1e-3
    )})
    
      set.seed(the_seed)
    timeCCD <- system.time({MNlogisticCCD <- mtool::mtool.MNlogisticCCD(
        X = data$X_train,
        Y = data$Y_train,
        offset = data$offset,
        N_covariates = unpen_cov,
        regularization = 'elastic-net',
        transpose = F,
        lambda1 = lambda1, lambda2 = lambda2, 
        lambda3 = 0,
    )})
    
    
    iter_vals <- map_dbl(MNlogistic$coefficientshist, ~
                             objective(matrix(.x, 
                                              nrow = ncol(data$X_train)), 
                                       data$X_train, data$Y_train, data$offset,
                                       lambda, alpha, 20))
    
    iter_vals_tol <- map_dbl(MNlogistic_tol$coefficientshist, ~
                                 objective(matrix(.x, 
                                                  nrow = ncol(data$X_train)), 
                                           data$X_train, data$Y_train, data$offset,
                                           lambda, alpha, 20))
    iter_valsExp <- map_dbl(MNlogisticExp$coefficientshist, ~
                                objective(matrix(.x, 
                                                 nrow = ncol(data$X_train)), 
                                          data$X_train, data$Y_train, data$offset,
                                          lambda, alpha, 20))
    
    iter_valsAcc <- map_dbl(MNlogisticAcc$coefficientshist, ~
                                objective(matrix(.x, 
                                                 nrow = ncol(data$X_train)), 
                                          data$X_train, data$Y_train, data$offset,
                                          lambda, alpha, 20))
    
     iter_valsSAHRA <- map_dbl(MNlogisticSAHRA$coefficientshist, ~
                                objective(matrix(.x, 
                                                 nrow = ncol(data$X_train)), 
                                          data$X_train, data$Y_train, data$offset,
                                          lambda, alpha, 20))
     
     iter_valsCCD <- map_dbl(MNlogisticCCD$coefficients, ~
                                 objective(matrix(.x, 
                                                  nrow = ncol(data$X_train)), 
                                           data$X_train, data$Y_train, data$offset,
                                           lambda, alpha, 20))
     
    # MNlogistic$coefficients
    # MNlogisticExp$coefficients
        if (i == 1) iter_sim <- tibble()
    
    iter_sim <- tibble(obj_step = iter_vals, 
                       iter = 1:length(iter_vals),
                       sim = i,
                       time = time[1],
                       algorithm = "Original implementation\n(tol = 1e-4)") %>% 
        bind_rows(
            # tibble(obj_step = iter_vals_tol, 
            #              iter = 1:length(iter_vals_tol),
            #              sim = i,
            #              time = time_tol[1],
            #              algorithm = "Original implementation \n(tol = 1e-5)"),
                  tibble(obj_step = iter_valsExp, 
                         iter = 1:length(iter_valsExp),
                         sim = i,
                         time = timeExp[1],
                         algorithm = "MNlogisticExp \n(Adj. param)"),
                  tibble(obj_step = iter_valsAcc, 
                         iter = 1:length(iter_valsAcc),
                         sim = i,
                         time = timeAcc[1],
                         algorithm = "Acc. MNlogisticExp"),
                  tibble(obj_step = iter_valsSAHRA, 
                         iter = 1:length(iter_valsSAHRA),
                         sim = i,
                         time = timeSAHRA[1],
                         algorithm = "SAHRA"),
                  tibble(obj_step = iter_valsCCD, 
                         iter = 1:length(iter_valsCCD),
                         sim = i,
                         time = timeCCD[1],
                         algorithm = "CCD")) %>% 
        bind_rows(iter_sim)
    saveRDS(iter_sim, here::here( "notes_jmr", "data", "opt-algorithms.rds"))
}
 

}

 iter_sim <- readRDS(here::here("notes_jmr", "data", "opt-algorithms.rds")) 
 
    iter_sim %>%  
        ggplot(aes(iter, y = obj_step, group = paste0(algorithm, sim),
                   color = str_wrap(algorithm, 15))) +
        # geom_line(alpha = 0.2) +
        stat_summary(aes(group = algorithm), geom = "point", fun.y = "mean", alpha = 0.5) +
        stat_summary(aes(group = algorithm), fun = mean, geom = "line", alpha = 0.5) +
        labs(y = "Obj. funct",
         x = "Epoch", 
         color = "") +
        xlim(0,9)
    
    iter_sim %>% 
        group_by(sim, algorithm) %>% 
        summarise(time = first(time)) %>% 
    ggplot(aes(x = algorithm, y = time)) +
        geom_boxplot() +
        labs(y = "Time",
             x = "Algorithm") 
 

```

Notes:

* Inner loop iterations do not appear to increase the number of epochs proportionally, suggesting that it may reduce computational time.  The inner loop iterations set to 2n appear to perform effectively, reducing computational time to 1/3. 
    * As the value of p increases, it seems that the algorithm fails to reach the minimum.
    * The situation becomes problematic when $p >> n$ because the initial epoch is below the tolerance level and far from the optimal position. To address this issue, we can lower the tolerance; however, this increases the number of steps required. By reducing the tolerance, we can observe that it becomes necessary to achieve reasonable coefficients in this context.
* **Tolerance** of 1e-4 in high-dimensional settings is not enough to reach optimal levels. 
    * Maybe, we can reduce the number of iterations as we increase the number of epochs. 
    * Under similar tolerance, an smaller tolerance seems to be better.
* **Learning rate**. The increase in learning rate from 1e-4 to 1e-3 reduces computational time by half. We should consider better learning rates. A smaller learning rate generates NaNs in the Frobenius norm.

Default parameters leads to premature stopping without achieving the optimal result. Using a smaller tolerance (green line) yields better outcomes, albeit requiring four times the computational time, as illustrated in the second plot.

By adjusting the parameters with a larger learning rate, fewer iterations in the inner loop, and maintaining the same tolerance (red line), we can achieve results comparable to those obtained with the smaller tolerance, but in a similar duration. However, this approach necessitates tuning for the specific case and, in some instances, can result in the algorithm getting stuck.

I also considered an accelerated version of SVRG and a variation known as SARAH. The accelerated version (blue line) integrates momentum techniques into the inner loop updates, aiming for faster convergence by accumulating velocity in favorable directions. This method demonstrates that each epoch progresses positively with additional steps while maintaining the same computational time as the other algorithms.

Conversely, SARAH (the orange line adjacent to the blue one) eliminates the necessity for repeated full gradient calculations after the first epoch by recursively updating the average gradient estimate using the differences between current and previous stochastic gradient iterates. As shown, it produces results similar to those of the accelerated version within the same timeframe.
In conclusion, while we could achieve better results within the same computational timeframe, aiming for faster convergence, we may require trying a different algorithm. 

